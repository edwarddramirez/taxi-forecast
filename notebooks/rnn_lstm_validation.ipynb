{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the source of the top 20 issues to make sure it isn't a widespread issue\n",
    "# Load the packages\n",
    "# Load both datasets\n",
    "# Transform both datasets into data loaders\n",
    "# Create multiple models that predict ride count\n",
    "    # Just ride count \n",
    "    # Ride count with other price, distance, etc\n",
    "    # Ride count, price, distance, and time variables\n",
    "    # Baseline model\n",
    "\n",
    "# Time variable model\n",
    "# Options:\n",
    "    # Embed the time variables\n",
    "\n",
    "# Model validation\n",
    "    # Take a model, the test dataset, then run the test dataset through the model to compute MSE or sMAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..') # add parent directory to path\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from utils import processing as pr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_size(distinct_values: int):\n",
    "    return min(50, (distinct_values + 1) // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_taxi_df = pd.read_pickle('data/test/adjusted_yellow_2022-01_2024-03_bypulocation.pkl')\n",
    "zero_taxi_df = pd.read_pickle('data/test/yellow_2022-01_2024-03_bypulocation.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset implementation\n",
    "import torch.utils\n",
    "\n",
    "# Datset paramaters\n",
    "# TODO: put these somewhere better\n",
    "batch_size = None  # Equal to the number of zones for convenience\n",
    "sequence_length = 96\n",
    "\n",
    "HOURS_PER_SERIES = 19627\n",
    "\n",
    "\n",
    "class TaxiDataset(torch.utils.data.Dataset):\n",
    "    # The dataset must have batch_size == num_zones\n",
    "    def __init__(\n",
    "            self, \n",
    "            taxi_data: pd.DataFrame, \n",
    "            sequence_length, \n",
    "            continuous_features=False, \n",
    "            time_features=False, \n",
    "            use_alternate_scaler=False, \n",
    "            alternate_scaler_source=None\n",
    "        ):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_zones = taxi_data[\"PULocationID\"].nunique()\n",
    "        self.zones = torch.tensor(list(range(self.num_zones))).to(device)\n",
    "        self.num_hours = taxi_data[\"pickup_datetime\"].nunique()\n",
    "        \n",
    "        # Simplest way to organize the data is to have all zones in a batch\n",
    "        self.batch_size = self.num_zones\n",
    "        self.num_continuous_features = 1\n",
    "        self.num_integer_features = 0\n",
    "\n",
    "        self.use_alternate_scaler=use_alternate_scaler \n",
    "        # self.alternate_scaler_source=alternate_scaler_source\n",
    "\n",
    "        taxi_data[\"counts\"] = taxi_data[\"counts\"].astype(np.float32)\n",
    "\n",
    "        # All of the preprocessing will happpen here\n",
    "        features_to_keep = [\"PULocationID\", \"pickup_datetime\", \"counts\"]\n",
    "        if continuous_features:\n",
    "            # Add continuous feature to keep list\n",
    "            continuous_features = ['tip_amount', 'fare_amount', 'trip_distance', 'trip_duration']\n",
    "            self.num_continuous_features += len(continuous_features)\n",
    "            features_to_keep.extend(continuous_features)\n",
    "        \n",
    "        if time_features:\n",
    "            # Create time features and add them to the keep list\n",
    "            # TODO add time features to dataset\n",
    "            time_features = ['pu_hour', 'pu_dayofweek', 'pu_month', 'counter']\n",
    "            self.num_integer_features = len(time_features)\n",
    "            features_to_keep.extend(time_features)\n",
    "\n",
    "        # SUBSET COLUMNS\n",
    "        # Remove any columns that aren't sued for sorting or in the model\n",
    "        taxi_data = taxi_data[features_to_keep]\n",
    "        \n",
    "        # DATA SCALING\n",
    "        if self.use_alternate_scaler:\n",
    "            self.count_scaler = alternate_scaler_source.count_scaler\n",
    "            self.other_scaler = alternate_scaler_source.other_scaler\n",
    "        else:\n",
    "            self.count_scaler = MinMaxScaler()\n",
    "            self.other_scaler = MinMaxScaler()\n",
    "        \n",
    "        taxi_data = self.fit_count_scaler(taxi_data)\n",
    "        if continuous_features:\n",
    "            taxi_data = self.fit_other_scaler(taxi_data)\n",
    "    \n",
    "        # Sort to prepare for splitting\n",
    "        taxi_data = taxi_data.sort_values([\"PULocationID\", \"pickup_datetime\"], ascending=True)\n",
    "        \n",
    "        # Separate out the integer\n",
    "        if self.num_integer_features:\n",
    "            int_data = taxi_data.loc[:, self.integer_features]\n",
    "            self.integer_features = self.features_to_tensor(int_data)\n",
    "            taxi_data = taxi_data.drop(self.integer_features, axis=1)\n",
    "        \n",
    "        # Drop batch_number, PULocationID, pickup_datetime\n",
    "        taxi_data = taxi_data.drop([\"PULocationID\", \"pickup_datetime\"], axis=1)\n",
    "        self.continuous_features = self.features_to_tensor(taxi_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        num_full_seqs = self.continuous_features.shape[0] // self.sequence_length\n",
    "        return self.num_zones * num_full_seqs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 0 gets zone 0 item 0\n",
    "        # 1 gets zone 1 item 0\n",
    "        # Num_zones = batch_size\n",
    "        # idx % (batch_size) = zone number\n",
    "        batch_idx, zone_idx = divmod(idx, self.batch_size)\n",
    "        cont_tensor = self.get_continuous_tensor(batch_idx, zone_idx)\n",
    "        target = self.get_target_tensor(batch_idx, zone_idx)\n",
    "        \n",
    "        zone_id_tensor = torch.tensor([zone_idx]).to(device)\n",
    "\n",
    "        if self.num_integer_features > 0:\n",
    "            int_tensor = self.get_integer_tensor(batch_idx, zone_idx)\n",
    "            return zone_id_tensor, cont_tensor, int_tensor, target\n",
    "        else:\n",
    "            return zone_id_tensor, cont_tensor, target\n",
    "\n",
    "    def set_max_sequence_length(self):\n",
    "        self.sequence_length = self.num_hours - 1\n",
    "\n",
    "    def get_continuous_tensor(self, batch_idx, zone_idx):\n",
    "        col_start = zone_idx * self.num_continuous_features\n",
    "        col_end = (zone_idx + 1) * self.num_continuous_features\n",
    "        row_start = batch_idx * self.sequence_length\n",
    "        row_end = (batch_idx + 1) * self.sequence_length\n",
    "        return self.continuous_features[row_start:row_end, col_start:col_end]\n",
    "    \n",
    "    def get_integer_tensor(self, batch_idx, zone_idx):\n",
    "        col_start = zone_idx * self.num_integer_features\n",
    "        col_end = (zone_idx + 1) * self.num_integer_features\n",
    "        row_start = batch_idx * self.sequence_length\n",
    "        row_end = (batch_idx + 1) * self.sequence_length\n",
    "        # print(\"Col Start: \", col_start)\n",
    "        # print(\"Col End  : \", col_end)\n",
    "        # print(\"Row Start: \", row_start)\n",
    "        # print(\"Row End  : \", row_end)\n",
    "        return self.integer_features[row_start:row_end, col_start:col_end]\n",
    "    \n",
    "    def get_target_tensor(self, batch_idx, zone_idx):\n",
    "        # TODO: Probably a lurking index out of bounds issue \n",
    "        # if data length % sequence_length == 0\n",
    "        col_idx = zone_idx * self.num_continuous_features\n",
    "        row_start = (batch_idx * self.sequence_length) + 1\n",
    "        row_end = ((batch_idx + 1) * self.sequence_length) + 1\n",
    "        return self.continuous_features[row_start:row_end, col_idx]\n",
    "\n",
    "    def features_to_tensor(self, features: pd.DataFrame):\n",
    "        result = features.to_numpy()\n",
    "        # TODO: May need a type conversion\n",
    "        # Split vertically into batches, then concat horizontally so \n",
    "        # time is along vertical axis and features are columns\n",
    "        v_split_out = np.vsplit(result, self.batch_size)\n",
    "        result = torch.tensor(np.hstack(v_split_out)).to(device)\n",
    "        return result\n",
    "    \n",
    "    def fit_count_scaler(self, taxi_data: pd.DataFrame):\n",
    "        value_cols = [\"counts\"]\n",
    "        counts = taxi_data.pivot(columns=\"PULocationID\", index=\"pickup_datetime\", values=value_cols)\n",
    "        counts_scaled = self.fit_scaler(self.count_scaler, counts)\n",
    "        return taxi_data.drop(value_cols, axis=1).merge(counts_scaled, on=[\"PULocationID\", \"pickup_datetime\"])\n",
    "\n",
    "    def fit_other_scaler(self, taxi_data: pd.DataFrame):\n",
    "        value_cols = ['tip_amount', 'fare_amount', 'trip_distance', 'trip_duration']\n",
    "        other_vars = taxi_data.pivot(columns=\"PULocationID\", index=\"pickup_datetime\", values=value_cols)\n",
    "        other_vars_scaled = self.fit_scaler(self.other_scaler, other_vars)\n",
    "        return taxi_data.drop(value_cols, axis=1).merge(other_vars_scaled, on=[\"PULocationID\", \"pickup_datetime\"])\n",
    "\n",
    "    def fit_scaler(self, scaler: MinMaxScaler, pivoted_data: pd.DataFrame):\n",
    "        # Time is along vertical axis\n",
    "        # The pivot columns should be in order but not sure what guarantees that\n",
    "        if self.use_alternate_scaler:\n",
    "            mat = scaler.transform(pivoted_data)\n",
    "        else:\n",
    "            mat = scaler.fit_transform(pivoted_data)\n",
    "        scaled = pd.DataFrame(mat)\n",
    "        scaled.columns = pivoted_data.columns\n",
    "        scaled.index = pivoted_data.index\n",
    "        scaled = scaled.stack(future_stack=True).reset_index()\n",
    "        return scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_timestamp = '2023-10-19 14:00:00-0400'\n",
    "def split_taxi_data_on_timestamp(taxi_dataset: pd.DataFrame, split_timestamp: str):\n",
    "    train_set = taxi_dataset[taxi_dataset[\"pickup_datetime\"] < split_timestamp].copy()\n",
    "    validation_set = taxi_dataset[taxi_dataset[\"pickup_datetime\"] >= split_timestamp].copy()\n",
    "    return train_set, validation_set\n",
    "\n",
    "def create_datasets(train_df, validation_df, sequence_length=24, continuous_features=False):\n",
    "    train_set = TaxiDataset(train_df, sequence_length, continuous_features)\n",
    "    validation_set = TaxiDataset(validation_df, sequence_length, continuous_features, use_alternate_scaler=True, alternate_scaler_source=train_set)\n",
    "    return train_set, validation_set\n",
    "\n",
    "def create_dataloaders(train_set, validation_set):\n",
    "    train_loader = DataLoader(train_set, batch_size=train_set.batch_size, shuffle=False, drop_last=True)\n",
    "    validation_loader = DataLoader(validation_set, batch_size=train_set.batch_size, shuffle=False, drop_last=True)\n",
    "    return train_loader, validation_loader\n",
    "\n",
    "def create_unbatched_loader(dataset: TaxiDataset):\n",
    "    dataset.set_max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = split_taxi_data_on_timestamp(zero_taxi_df, split_timestamp)\n",
    "train_set, validation_set = create_datasets(train_df, validation_df, sequence_length, continuous_features=True)\n",
    "train_loader, validation_loader = create_dataloaders(train_set, validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelType(Enum):\n",
    "    RNN = 1\n",
    "    LSTM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "class BaselineModel(nn.Module):\n",
    "    savable = False\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.fc = nn.Linear(3, 10)\n",
    "    \n",
    "    def forward(self, _, continuous: torch.tensor):\n",
    "        constant_pred = continuous[:, :, 0].detach().clone()\n",
    "        constant_pred.requires_grad_()\n",
    "        return constant_pred\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single variable\n",
    "class MultiSeriesModel(nn.Module):\n",
    "    savable = True\n",
    "    def __init__(\n",
    "            self, \n",
    "            zone_count, \n",
    "            model_type: ModelType, \n",
    "            batch_size: int, \n",
    "            input_size=1, \n",
    "            hidden_size=50, \n",
    "            output_size=1, \n",
    "            num_layers=1\n",
    "        ):\n",
    "        super(MultiSeriesModel, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.embed_size = embed_size(zone_count)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.zone_embed = nn.Embedding(zone_count, self.embed_size)\n",
    "        \n",
    "        if self.model_type == ModelType.RNN:\n",
    "            self.cell = nn.RNN(self.embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.h = torch.zeros(num_layers, batch_size, hidden_size, device=device)\n",
    "        elif self.model_type == ModelType.LSTM:\n",
    "            self.cell = nn.LSTM(self.embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.h = [torch.zeros(num_layers, batch_size, hidden_size, device=device) for _ in range(2)]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model only supports RNN and LSTM\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, zones, continuous):\n",
    "        # Zones is (zone_count, 1), emb (zone_count, embed_size(zone_count))\n",
    "        sequence_length = continuous.shape[-2]\n",
    "        embed_result = self.zone_embed(zones).expand((-1, sequence_length, -1))\n",
    "        x = torch.cat([embed_result, continuous], dim=-1)\n",
    "        if self.model_type == ModelType.RNN:\n",
    "            out, h = self.cell(x, self.h)\n",
    "            self.h = h.detach()\n",
    "        elif self.model_type == ModelType.LSTM:\n",
    "            out, h = self.cell(x, self.h)\n",
    "            self.h = [h_.detach() for h_ in h]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model only supports RNN and LSTM\")\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out.squeeze()\n",
    "\n",
    "    def reset(self):\n",
    "        if type(self.h) is list:\n",
    "            for h in self.h: \n",
    "                h.zero_()\n",
    "        else:\n",
    "            self.h.zero_()\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        if self.input_size > 1:\n",
    "            return f\"{self.model_type.name}_MultiSeriesModel\"\n",
    "        else:\n",
    "            return f\"{self.model_type.name}_SingleSeriesModel\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = MultiSeriesModel(zone_count, ModelType.LSTM, train_set.batch_size, train_set.num_continuous_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = BaselineModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple and time series variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLoop:\n",
    "    NUM_EPOCHS = 1000\n",
    "    MODEL_FOLDER = \"../models/model_validation/\"\n",
    "    PRINT_EVERY = 10\n",
    "\n",
    "    def __init__(self, model, train_loader, validation_loader):\n",
    "        self.model: nn.Module = model\n",
    "        self.optimizer: optim.Optimizer = optim.Adam(model.parameters(), lr=3e-3)\n",
    "        self.criterion: nn.MSELoss = nn.MSELoss()\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "        self.min_valid_loss = float(\"inf\")\n",
    "        self.best_model_path = \"\"\n",
    "\n",
    "        self.valid_loss_list = []\n",
    "        self.consecutive_loss_increases = 0\n",
    "\n",
    "\n",
    "    def train_epoch(self) -> float:\n",
    "        self.model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for *vars, target in self.train_loader:\n",
    "            preds = self.model(*vars)\n",
    "            train_loss = self.criterion(preds, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_train_loss += train_loss.item()\n",
    "        self.model.reset()\n",
    "\n",
    "        avg_train_loss = epoch_train_loss / len(self.train_loader)\n",
    "        return avg_train_loss\n",
    "\n",
    "    def validate_epoch(self) -> float:\n",
    "        self.model.eval()\n",
    "        epoch_valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for *vars, target in self.validation_loader:\n",
    "                preds = self.model(*vars)            \n",
    "                valid_loss = self.criterion(preds, target)\n",
    "\n",
    "                epoch_valid_loss += valid_loss.item()\n",
    "        self.model.reset()\n",
    "        \n",
    "        avg_valid_loss = epoch_valid_loss / len(self.validation_loader)\n",
    "        return avg_valid_loss\n",
    "    \n",
    "    def print_loss(self, epoch, train_loss, valid_loss):\n",
    "        print(f'Epoch [{epoch + 1:04}/{self.NUM_EPOCHS}], Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "    \n",
    "    def fit_one_epoch(self):\n",
    "        avg_train_loss = self.train_epoch()\n",
    "        avg_valid_loss = self.validate_epoch()\n",
    "        return avg_train_loss, avg_valid_loss\n",
    "    \n",
    "    def save_model(self, epoch) -> str:\n",
    "        if self.model.savable:\n",
    "            model_name = f\"{self.model.get_model_name()}_{self.timestamp}_{epoch}\"\n",
    "            model_path = self.MODEL_FOLDER + model_name\n",
    "            torch.save(self.model.state_dict(), model_path)\n",
    "            return model_path\n",
    "\n",
    "    def handle_early_stopping(self, epoch) -> bool:\n",
    "        stop = False\n",
    "        if self.valid_loss_list[-1] < self.min_valid_loss:\n",
    "            # Save model\n",
    "            self.min_valid_loss = self.valid_loss_list[-1]\n",
    "            self.best_model_path = self.save_model(epoch)\n",
    "            # Reset count\n",
    "            self.consecutive_loss_increases = 0\n",
    "        else:\n",
    "            self.consecutive_loss_increases += 1\n",
    "            if self.consecutive_loss_increases >= 20:\n",
    "                stop = True\n",
    "\n",
    "        return stop\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.NUM_EPOCHS):\n",
    "            avg_train_loss, avg_valid_loss = self.fit_one_epoch()\n",
    "\n",
    "            if (epoch + 1) % self.PRINT_EVERY == 0:\n",
    "                self.print_loss(epoch, avg_train_loss, avg_valid_loss)\n",
    "\n",
    "            ## Early Stopping: check if the error went up or down\n",
    "            self.valid_loss_list.append(avg_valid_loss)\n",
    "            stop = self.handle_early_stopping(epoch)\n",
    "            if stop or not self.model.savable:\n",
    "                break\n",
    "        if self.model.savable:\n",
    "            self.model.load_state_dict(torch.load(self.best_model_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop = TrainingLoop(test_model, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop = TrainingLoop(base_model, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0050, Validation Loss: 0.0070\n",
      "Epoch [0020/1000], Train Loss: 0.0047, Validation Loss: 0.0067\n",
      "Epoch [0030/1000], Train Loss: 0.0044, Validation Loss: 0.0065\n",
      "Epoch [0040/1000], Train Loss: 0.0043, Validation Loss: 0.0065\n",
      "Epoch [0050/1000], Train Loss: 0.0041, Validation Loss: 0.0065\n"
     ]
    }
   ],
   "source": [
    "# loop.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008764609"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better than baseline !!!\n",
    "# np.mean(np.power(np.array(train_set.continuous_features.cpu()[:-1, :] - train_set.continuous_features.cpu()[1:, :]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            train_set: TaxiDataset,\n",
    "            validation_set: TaxiDataset\n",
    "        ) -> None:\n",
    "        validation_set.set_max_sequence_length()\n",
    "        self.train_set = train_set\n",
    "        self.validation_set = validation_set\n",
    "        self.validation_loader = DataLoader(validation_set, validation_set.batch_size, shuffle=False)\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "        # TODO: Validation set needs to be scaled by train set scalers\n",
    "        \n",
    "    def get_preds(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for *vars, target in self.validation_loader:\n",
    "                preds = self.model(*vars)\n",
    "        self.model.reset()\n",
    "        # Transpose so that time is along vertical\n",
    "        preds = preds.cpu().detach().numpy().T\n",
    "        target = target.cpu().detach().numpy().T\n",
    "        \n",
    "        return preds, target\n",
    "    \n",
    "    def unscale_preds_and_target(self, scaled_preds, scaled_target):\n",
    "        preds = self.train_set.count_scaler.inverse_transform(scaled_preds)\n",
    "        target = self.validation_set.count_scaler.inverse_transform(scaled_target)\n",
    "        return preds, target\n",
    "\n",
    "    def mse_loss(self, preds, target):\n",
    "        # mse_loss = nn.MSELoss()\n",
    "        # scaled_loss = mse_loss(preds, target)\n",
    "        # print(type(scaled_loss))\n",
    "        # return scaled_loss.item()\n",
    "        return ((preds - target) ** 2).mean()\n",
    "\n",
    "    def smape_loss(self, preds, target):\n",
    "        # TODO: Eliminiate runtime warning when denominator == 0\n",
    "        smape_mat = (2 * np.abs(preds - target)) / (np.abs(preds) + np.abs(target))\n",
    "        smape_mat = np.mean(np.nan_to_num(smape_mat))\n",
    "        return smape_mat\n",
    "\n",
    "    def validate(self):\n",
    "        scaled_preds, scaled_target = self.get_preds()\n",
    "        preds, target = self.unscale_preds_and_target(scaled_preds, scaled_target)\n",
    "        \n",
    "        self.preds = preds\n",
    "        self.target = target\n",
    "        self.scaled_mse_loss = self.mse_loss(scaled_preds, scaled_target)\n",
    "        self.unscaled_mse_loss = self.mse_loss(preds, target)\n",
    "        self.smape_loss = self.smape_loss(preds, target)\n",
    "\n",
    "    def print_results(self):\n",
    "        print(f\"Scaled MSE: {self.scaled_mse_loss:.4f}, Unscaled MSE: {self.unscaled_mse_loss}, sMAPE: {self.smape_loss}\")\n",
    "    \n",
    "    # TODO: Some kind of plot support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This validation set has its own scaler to get back out the actual data\n",
    "# valid = Validator(loop.model, train_set, validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noahj\\AppData\\Local\\Temp\\ipykernel_42124\\2039738778.py:42: RuntimeWarning: invalid value encountered in divide\n",
      "  smape_mat = (2 * np.abs(preds - target)) / (np.abs(preds) + np.abs(target))\n"
     ]
    }
   ],
   "source": [
    "# valid.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled MSE: 0.0101, Unscaled MSE: 680.8367309570312, sMAPE: 0.5200790166854858\n"
     ]
    }
   ],
   "source": [
    "# valid.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelContainer:\n",
    "    # Act as a container for all of the other models\n",
    "    def __init__(self, taxi_df, model_type, sequence_length) -> None:\n",
    "        # Configuration options\n",
    "        self.continuous_features = True\n",
    "        time_features = False\n",
    "\n",
    "        self.data = taxi_df\n",
    "        self.model_type = model_type\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Setup\n",
    "        self.create_datasets()\n",
    "        self.setup_model()\n",
    "\n",
    "    def run(self):\n",
    "        self.train_model()\n",
    "        self.validate_model()\n",
    "\n",
    "    def create_datasets(self):\n",
    "        train_df, validation_df = split_taxi_data_on_timestamp(self.data, split_timestamp)\n",
    "        train_set, validation_set = create_datasets(train_df, validation_df, self.sequence_length, continuous_features=self.continuous_features)\n",
    "\n",
    "        self.train_set = train_set\n",
    "\n",
    "        train_loader, validation_loader = create_dataloaders(train_set, validation_set)\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "\n",
    "        self.validation_set_for_validation = TaxiDataset(validation_df, self.sequence_length, continuous_features=self.continuous_features)\n",
    "\n",
    "    def setup_model(self):\n",
    "        if self.model_type == 'baseline':\n",
    "            self.model = BaselineModel()\n",
    "        elif self.model_type == 'rnn':\n",
    "            self.model = MultiSeriesModel(train_set.num_zones, ModelType.RNN, train_set.batch_size, train_set.num_continuous_features).to(device)\n",
    "        elif self.model_type == 'lstm':\n",
    "            self.model = MultiSeriesModel(train_set.num_zones, ModelType.LSTM, train_set.batch_size, train_set.num_continuous_features).to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"mode_type {self.model_type} not supported\")\n",
    "\n",
    "    def train_model(self):\n",
    "        loop = TrainingLoop(self.model, self.train_loader, self.validation_loader)\n",
    "        loop.train()\n",
    "        \n",
    "    def validate_model(self):\n",
    "        valid = Validator(self.model, self.train_set, self.validation_set_for_validation)\n",
    "        valid.validate()\n",
    "        valid.print_results()\n",
    "    \n",
    "    # Dataset generation\n",
    "    # Create the Model\n",
    "    # Fit the model\n",
    "    # Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ModelContainer(mean_taxi_df, 'baseline', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled MSE: 0.0101, Unscaled MSE: 680.8367309570312, sMAPE: 0.5200790166854858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noahj\\AppData\\Local\\Temp\\ipykernel_35280\\116614121.py:43: RuntimeWarning: invalid value encountered in divide\n",
      "  smape_mat = (2 * np.abs(preds - target)) / (np.abs(preds) + np.abs(target))\n"
     ]
    }
   ],
   "source": [
    "test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0076, Validation Loss: 0.0075\n",
      "Epoch [0020/1000], Train Loss: 0.0067, Validation Loss: 0.0066\n",
      "Epoch [0030/1000], Train Loss: 0.0057, Validation Loss: 0.0056\n",
      "Epoch [0040/1000], Train Loss: 0.0054, Validation Loss: 0.0054\n",
      "Epoch [0050/1000], Train Loss: 0.0052, Validation Loss: 0.0052\n",
      "Epoch [0060/1000], Train Loss: 0.0052, Validation Loss: 0.0051\n",
      "Epoch [0070/1000], Train Loss: 0.0050, Validation Loss: 0.0050\n",
      "Epoch [0080/1000], Train Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch [0090/1000], Train Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch [0100/1000], Train Loss: 0.0050, Validation Loss: 0.0049\n",
      "Epoch [0110/1000], Train Loss: 0.0048, Validation Loss: 0.0049\n",
      "Epoch [0120/1000], Train Loss: 0.0048, Validation Loss: 0.0048\n",
      "Epoch [0130/1000], Train Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch [0140/1000], Train Loss: 0.0047, Validation Loss: 0.0047\n",
      "Epoch [0150/1000], Train Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch [0160/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0170/1000], Train Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch [0180/1000], Train Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch [0190/1000], Train Loss: 0.0045, Validation Loss: 0.0045\n",
      "Epoch [0200/1000], Train Loss: 0.0045, Validation Loss: 0.0045\n",
      "Epoch [0210/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0220/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0230/1000], Train Loss: 0.0044, Validation Loss: 0.0044\n",
      "Epoch [0240/1000], Train Loss: 0.0044, Validation Loss: 0.0044\n",
      "Epoch [0250/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0260/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0270/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0280/1000], Train Loss: 0.0043, Validation Loss: 0.0043\n",
      "Epoch [0290/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0300/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0310/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0320/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0330/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch [0340/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0350/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch [0360/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0370/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0380/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0390/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0400/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0410/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0420/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0430/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0440/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0450/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0460/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Scaled MSE: 0.0058, Unscaled MSE: 371.2231750488281, sMAPE: 0.6074097752571106\n"
     ]
    }
   ],
   "source": [
    "test1 = ModelContainer(mean_taxi_df, 'lstm', 1000)\n",
    "test1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0075, Validation Loss: 0.0074\n",
      "Epoch [0020/1000], Train Loss: 0.0062, Validation Loss: 0.0060\n",
      "Epoch [0030/1000], Train Loss: 0.0056, Validation Loss: 0.0055\n",
      "Epoch [0040/1000], Train Loss: 0.0053, Validation Loss: 0.0053\n",
      "Epoch [0050/1000], Train Loss: 0.0052, Validation Loss: 0.0052\n",
      "Epoch [0060/1000], Train Loss: 0.0051, Validation Loss: 0.0051\n",
      "Epoch [0070/1000], Train Loss: 0.0050, Validation Loss: 0.0050\n",
      "Epoch [0080/1000], Train Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch [0090/1000], Train Loss: 0.0049, Validation Loss: 0.0049\n",
      "Epoch [0100/1000], Train Loss: 0.0048, Validation Loss: 0.0049\n",
      "Epoch [0110/1000], Train Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch [0120/1000], Train Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch [0130/1000], Train Loss: 0.0047, Validation Loss: 0.0047\n",
      "Epoch [0140/1000], Train Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch [0150/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0160/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0170/1000], Train Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch [0180/1000], Train Loss: 0.0047, Validation Loss: 0.0046\n",
      "Epoch [0190/1000], Train Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch [0200/1000], Train Loss: 0.0045, Validation Loss: 0.0045\n",
      "Epoch [0210/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0220/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0230/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0240/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0250/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0260/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0270/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0280/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0290/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0300/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0310/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0320/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0330/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch [0340/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch [0350/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0360/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Scaled MSE: 0.0061, Unscaled MSE: 391.5155944824219, sMAPE: 0.6256916522979736\n"
     ]
    }
   ],
   "source": [
    "test2 = ModelContainer(zero_taxi_df, 'lstm', 1000)\n",
    "test2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_and_validation_sets(taxi_df, split_timestamp):\n",
    "    train_df, validation_df = split_taxi_data_on_timestamp(taxi_df, split_timestamp)\n",
    "    train_set, validation_set = create_datasets(train_df, validation_df, sequence_length, continuous_features=True)\n",
    "    return train_set, validation_set\n",
    "\n",
    "def setup_models(train_set: TaxiDataset):\n",
    "    baseline = BaselineModel()\n",
    "    rnn = MultiSeriesModel(train_set.num_zones, ModelType.RNN, train_set.batch_size, train_set.num_continuous_features).to(device)\n",
    "    lstm = MultiSeriesModel(train_set.num_zones, ModelType.LSTM, train_set.batch_size, train_set.num_continuous_features).to(device)\n",
    "    models = [baseline, rnn, lstm]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(train_set):\n",
    "    models = setup_models(train_set)\n",
    "    return models\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, validation_loader):\n",
    "    # The models should have the best parameters by the end\n",
    "    loop = TrainingLoop(model, train_loader, validation_loader)\n",
    "    loop.train()\n",
    "    return loop\n",
    "\n",
    "\n",
    "def validate_models(model, train_set, taxi_df, split_timestamp, continuous_features):\n",
    "    _, validation_df = split_taxi_data_on_timestamp(taxi_df, split_timestamp)\n",
    "    validation_set = TaxiDataset(validation_df, sequence_length, continuous_features)\n",
    "    valid = Validator(model, train_set, validation_set)\n",
    "    valid.validate()\n",
    "    valid.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, validation_set = create_train_and_validation_sets(mean_taxi_df, split_timestamp)\n",
    "train_loader, validation_loader = create_dataloaders(train_set, validation_set)\n",
    "models = create_models(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = BaselineModel().to(device)\n",
    "rnn = MultiSeriesModel(train_set.num_zones, ModelType.RNN, train_set.batch_size, train_set.num_continuous_features).to(device)\n",
    "lstm = MultiSeriesModel(train_set.num_zones, ModelType.LSTM, train_set.batch_size, train_set.num_continuous_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_single.num_continuous_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = split_taxi_data_on_timestamp(mean_taxi_df, split_timestamp)\n",
    "train_set_single, validation_set_single = create_datasets(train_df, validation_df, sequence_length)\n",
    "train_loader_single, validation_loader_single = create_dataloaders(train_set_single, validation_set_single)\n",
    "lstm_single = MultiSeriesModel(train_set.num_zones, ModelType.LSTM, train_set_single.batch_size, train_set_single.num_continuous_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0049, Validation Loss: 0.0049\n",
      "Epoch [0020/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0030/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0040/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0050/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch [0060/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0070/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0080/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0090/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0100/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0110/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0120/1000], Train Loss: 0.0039, Validation Loss: 0.0042\n",
      "Epoch [0130/1000], Train Loss: 0.0038, Validation Loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "lstm_single_loop = train_model(lstm_single, train_loader_single, validation_loader_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_loop = train_model(baseline, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_models(baseline, train_set_single, mean_taxi_df, split_timestamp, continuous_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0057, Validation Loss: 0.0058\n",
      "Epoch [0020/1000], Train Loss: 0.0052, Validation Loss: 0.0054\n",
      "Epoch [0030/1000], Train Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch [0040/1000], Train Loss: 0.0047, Validation Loss: 0.0049\n",
      "Epoch [0050/1000], Train Loss: 0.0046, Validation Loss: 0.0049\n",
      "Epoch [0060/1000], Train Loss: 0.0046, Validation Loss: 0.0048\n",
      "Epoch [0070/1000], Train Loss: 0.0045, Validation Loss: 0.0048\n",
      "Epoch [0080/1000], Train Loss: 0.0045, Validation Loss: 0.0047\n",
      "Epoch [0090/1000], Train Loss: 0.0044, Validation Loss: 0.0047\n",
      "Epoch [0100/1000], Train Loss: 0.0044, Validation Loss: 0.0046\n",
      "Epoch [0110/1000], Train Loss: 0.0044, Validation Loss: 0.0048\n",
      "Epoch [0120/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0130/1000], Train Loss: 0.0044, Validation Loss: 0.0046\n",
      "Epoch [0140/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0150/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0160/1000], Train Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch [0170/1000], Train Loss: 0.0044, Validation Loss: 0.0046\n"
     ]
    }
   ],
   "source": [
    "rnn_loop = train_model(rnn, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0049, Validation Loss: 0.0049\n",
      "Epoch [0020/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0030/1000], Train Loss: 0.0044, Validation Loss: 0.0044\n",
      "Epoch [0040/1000], Train Loss: 0.0043, Validation Loss: 0.0043\n",
      "Epoch [0050/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0060/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0070/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0080/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0090/1000], Train Loss: 0.0039, Validation Loss: 0.0042\n",
      "Epoch [0100/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0110/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0120/1000], Train Loss: 0.0038, Validation Loss: 0.0041\n",
      "Epoch [0130/1000], Train Loss: 0.0038, Validation Loss: 0.0041\n",
      "Epoch [0140/1000], Train Loss: 0.0038, Validation Loss: 0.0041\n",
      "Epoch [0150/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch [0160/1000], Train Loss: 0.0038, Validation Loss: 0.0041\n",
      "Epoch [0170/1000], Train Loss: 0.0037, Validation Loss: 0.0040\n"
     ]
    }
   ],
   "source": [
    "lstm_loop = train_model(lstm, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_loop = train_model(lstm, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled MSE: 0.0051, Unscaled MSE: 274.3553771972656, sMAPE: 0.5782305598258972\n"
     ]
    }
   ],
   "source": [
    "validate_models(lstm_single, train_set_single, mean_taxi_df, split_timestamp, continuous_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled MSE: 0.0063, Unscaled MSE: 390.16009521484375, sMAPE: 0.6303727626800537\n"
     ]
    }
   ],
   "source": [
    "validate_models(rnn, train_set, mean_taxi_df, split_timestamp, continuous_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled MSE: 0.0058, Unscaled MSE: 342.6379699707031, sMAPE: 0.5724579691886902\n"
     ]
    }
   ],
   "source": [
    "validate_models(lstm, train_set, mean_taxi_df, split_timestamp, continuous_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_set = TaxiDataset(validation_df, sequence_length, continuous_features)\n",
    "valid = Validator(model, train_set, validation_set)\n",
    "valid.validate()\n",
    "valid.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
