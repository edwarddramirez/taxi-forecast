{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the source of the top 20 issues to make sure it isn't a widespread issue\n",
    "# Load the packages\n",
    "# Load both datasets\n",
    "# Transform both datasets into data loaders\n",
    "# Create multiple models that predict ride count\n",
    "    # Just ride count \n",
    "    # Ride count with other price, distance, etc\n",
    "    # Ride count, price, distance, and time variables\n",
    "    # Baseline model\n",
    "\n",
    "# Time variable model\n",
    "# Options:\n",
    "    # Embed the time variables\n",
    "\n",
    "# Model validation\n",
    "    # Take a model, the test dataset, then run the test dataset through the model to compute MSE or sMAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..') # add parent directory to path\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from utils import processing as pr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_size(distinct_values: int):\n",
    "    return min(50, (distinct_values + 1) // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_taxi_df = pd.read_pickle('data/test/adjusted_yellow_2022-01_2024-03_bypulocation.pkl')\n",
    "zero_taxi_df = pd.read_pickle('data/test/yellow_2022-01_2024-03_bypulocation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_taxi_df.to_csv(\"./taxi_actual.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset implementation\n",
    "import torch.utils\n",
    "\n",
    "# Datset paramaters\n",
    "# TODO: put these somewhere better\n",
    "batch_size = None  # Equal to the number of zones for convenience\n",
    "sequence_length = 96\n",
    "\n",
    "HOURS_PER_SERIES = 19627\n",
    "\n",
    "\n",
    "class TaxiDataset(torch.utils.data.Dataset):\n",
    "    # The dataset must have batch_size == num_zones\n",
    "    def __init__(\n",
    "            self, \n",
    "            taxi_data: pd.DataFrame, \n",
    "            sequence_length, \n",
    "            continuous_features=False, \n",
    "            time_features=False, \n",
    "            use_alternate_scaler=False, \n",
    "            alternate_scaler_source=None\n",
    "        ):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_zones = taxi_data[\"PULocationID\"].nunique()\n",
    "        self.zones = torch.tensor(list(range(self.num_zones))).to(device)\n",
    "        self.num_hours = taxi_data[\"pickup_datetime\"].nunique()\n",
    "        \n",
    "        # Simplest way to organize the data is to have all zones in a batch\n",
    "        self.batch_size = self.num_zones\n",
    "        self.num_continuous_features = 1\n",
    "        self.num_integer_features = 0\n",
    "\n",
    "        self.use_alternate_scaler = use_alternate_scaler \n",
    "        # self.alternate_scaler_source=alternate_scaler_source\n",
    "\n",
    "        taxi_data[\"counts\"] = taxi_data[\"counts\"].astype(np.float32)\n",
    "\n",
    "        # All of the preprocessing will happpen here\n",
    "        features_to_keep = [\"PULocationID\", \"pickup_datetime\", \"counts\"]\n",
    "        if continuous_features:\n",
    "            # Add continuous feature to keep list\n",
    "            continuous_features = ['tip_amount', 'fare_amount', 'trip_distance', 'trip_duration']\n",
    "            self.num_continuous_features += len(continuous_features)\n",
    "            features_to_keep.extend(continuous_features)\n",
    "        \n",
    "        if time_features:\n",
    "            # Create time features and add them to the keep list\n",
    "            # TODO add time features to dataset\n",
    "            taxi_data = self.add_time_features(taxi_data)\n",
    "            time_features = ['pu_hour', 'pu_dayofweek', 'pu_month']\n",
    "            self.num_integer_features = len(time_features)\n",
    "            features_to_keep.extend(time_features)\n",
    "\n",
    "        # SUBSET COLUMNS\n",
    "        # Remove any columns that aren't sued for sorting or in the model\n",
    "        taxi_data = taxi_data[features_to_keep]\n",
    "        \n",
    "        # DATA SCALING\n",
    "        if self.use_alternate_scaler:\n",
    "            self.count_scaler = alternate_scaler_source.count_scaler\n",
    "            self.other_scaler = alternate_scaler_source.other_scaler\n",
    "        else:\n",
    "            self.count_scaler = MinMaxScaler()\n",
    "            self.other_scaler = MinMaxScaler()\n",
    "        \n",
    "        taxi_data = self.fit_count_scaler(taxi_data)\n",
    "        if continuous_features:\n",
    "            taxi_data = self.fit_other_scaler(taxi_data)\n",
    "    \n",
    "        # Sort to prepare for splitting\n",
    "        taxi_data = taxi_data.sort_values([\"PULocationID\", \"pickup_datetime\"], ascending=True)\n",
    "        \n",
    "        # Separate out the integer\n",
    "        if self.num_integer_features:\n",
    "            int_data = taxi_data.loc[:, time_features]\n",
    "            self.integer_features = self.features_to_tensor(int_data)\n",
    "            taxi_data = taxi_data.drop(time_features, axis=1)\n",
    "            self.int_features_unique = {}\n",
    "            for idx in range(self.num_integer_features):\n",
    "                self.int_features_unique[idx] = len(self.integer_features[:, idx].unique())\n",
    "        \n",
    "        # Drop batch_number, PULocationID, pickup_datetime\n",
    "        taxi_data = taxi_data.drop([\"PULocationID\", \"pickup_datetime\"], axis=1)\n",
    "        self.continuous_features = self.features_to_tensor(taxi_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        num_full_seqs = self.continuous_features.shape[0] // self.sequence_length\n",
    "        return self.num_zones * num_full_seqs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 0 gets zone 0 item 0\n",
    "        # 1 gets zone 1 item 0\n",
    "        # Num_zones = batch_size\n",
    "        # idx % (batch_size) = zone number\n",
    "        batch_idx, zone_idx = divmod(idx, self.batch_size)\n",
    "        cont_tensor = self.get_continuous_tensor(batch_idx, zone_idx)\n",
    "        target = self.get_target_tensor(batch_idx, zone_idx)\n",
    "        \n",
    "        zone_id_tensor = torch.tensor([zone_idx]).to(device)\n",
    "\n",
    "        if self.num_integer_features > 0:\n",
    "            int_tensor = self.get_integer_tensor(batch_idx, zone_idx)\n",
    "            return zone_id_tensor, cont_tensor, int_tensor, target\n",
    "        else:\n",
    "            return zone_id_tensor, cont_tensor, target\n",
    "\n",
    "    def set_max_sequence_length(self):\n",
    "        self.sequence_length = self.num_hours - 1\n",
    "\n",
    "    def add_time_features(self, taxi_data):\n",
    "        taxi_data['pu_hour'] = taxi_data['pickup_datetime'].dt.hour\n",
    "        taxi_data['pu_hour'] = taxi_data['pu_hour'] - np.min(taxi_data['pu_hour'])\n",
    "        taxi_data['pu_dayofweek'] = taxi_data['pickup_datetime'].dt.dayofweek\n",
    "        taxi_data['pu_dayofweek'] = taxi_data['pu_dayofweek'] - np.min(taxi_data['pu_dayofweek'])\n",
    "        taxi_data['pu_month'] = taxi_data['pickup_datetime'].dt.month\n",
    "        taxi_data['pu_month'] = taxi_data['pu_month'] - np.min(taxi_data['pu_month'])\n",
    "        return taxi_data\n",
    "\n",
    "    def get_continuous_tensor(self, batch_idx, zone_idx):\n",
    "        col_start = zone_idx * self.num_continuous_features\n",
    "        col_end = (zone_idx + 1) * self.num_continuous_features\n",
    "        row_start = batch_idx * self.sequence_length\n",
    "        row_end = (batch_idx + 1) * self.sequence_length\n",
    "        return self.continuous_features[row_start:row_end, col_start:col_end]\n",
    "    \n",
    "    def get_integer_tensor(self, batch_idx, zone_idx):\n",
    "        col_start = zone_idx * self.num_integer_features\n",
    "        col_end = (zone_idx + 1) * self.num_integer_features\n",
    "        row_start = batch_idx * self.sequence_length\n",
    "        row_end = (batch_idx + 1) * self.sequence_length\n",
    "        # print(\"Col Start: \", col_start)\n",
    "        # print(\"Col End  : \", col_end)\n",
    "        # print(\"Row Start: \", row_start)\n",
    "        # print(\"Row End  : \", row_end)\n",
    "        return self.integer_features[row_start:row_end, col_start:col_end]\n",
    "    \n",
    "    def get_target_tensor(self, batch_idx, zone_idx):\n",
    "        # TODO: Probably a lurking index out of bounds issue \n",
    "        # if data length % sequence_length == 0\n",
    "        col_idx = zone_idx * self.num_continuous_features\n",
    "        row_start = (batch_idx * self.sequence_length) + 1\n",
    "        row_end = ((batch_idx + 1) * self.sequence_length) + 1\n",
    "        return self.continuous_features[row_start:row_end, col_idx]\n",
    "\n",
    "    def features_to_tensor(self, features: pd.DataFrame):\n",
    "        result = features.to_numpy()\n",
    "        # TODO: May need a type conversion\n",
    "        # Split vertically into batches, then concat horizontally so \n",
    "        # time is along vertical axis and features are columns\n",
    "        v_split_out = np.vsplit(result, self.batch_size)\n",
    "        result = torch.tensor(np.hstack(v_split_out)).to(device)\n",
    "        return result\n",
    "    \n",
    "    def fit_count_scaler(self, taxi_data: pd.DataFrame):\n",
    "        value_cols = [\"counts\"]\n",
    "        counts = taxi_data.pivot(columns=\"PULocationID\", index=\"pickup_datetime\", values=value_cols)\n",
    "        counts_scaled = self.fit_scaler(self.count_scaler, counts)\n",
    "        return taxi_data.drop(value_cols, axis=1).merge(counts_scaled, on=[\"PULocationID\", \"pickup_datetime\"])\n",
    "\n",
    "    def fit_other_scaler(self, taxi_data: pd.DataFrame):\n",
    "        value_cols = ['tip_amount', 'fare_amount', 'trip_distance', 'trip_duration']\n",
    "        other_vars = taxi_data.pivot(columns=\"PULocationID\", index=\"pickup_datetime\", values=value_cols)\n",
    "        other_vars_scaled = self.fit_scaler(self.other_scaler, other_vars)\n",
    "        return taxi_data.drop(value_cols, axis=1).merge(other_vars_scaled, on=[\"PULocationID\", \"pickup_datetime\"])\n",
    "\n",
    "    def fit_scaler(self, scaler: MinMaxScaler, pivoted_data: pd.DataFrame):\n",
    "        # Time is along vertical axis\n",
    "        # The pivot columns should be in order but not sure what guarantees that\n",
    "        if self.use_alternate_scaler:\n",
    "            mat = scaler.transform(pivoted_data)\n",
    "        else:\n",
    "            mat = scaler.fit_transform(pivoted_data)\n",
    "        scaled = pd.DataFrame(mat)\n",
    "        scaled.columns = pivoted_data.columns\n",
    "        scaled.index = pivoted_data.index\n",
    "        scaled = scaled.stack(future_stack=True).reset_index()\n",
    "        return scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_timestamp = '2023-10-19 14:00:00-0400'\n",
    "def split_taxi_data_on_timestamp(taxi_dataset: pd.DataFrame, split_timestamp: str):\n",
    "    train_set = taxi_dataset[taxi_dataset[\"pickup_datetime\"] < split_timestamp].copy()\n",
    "    validation_set = taxi_dataset[taxi_dataset[\"pickup_datetime\"] >= split_timestamp].copy()\n",
    "    return train_set, validation_set\n",
    "\n",
    "def create_datasets(train_df, validation_df, sequence_length=24, continuous_features=False, time_features=False):\n",
    "    train_set = TaxiDataset(train_df, sequence_length, continuous_features, time_features)\n",
    "    validation_set = TaxiDataset(validation_df, sequence_length, continuous_features, time_features, use_alternate_scaler=True, alternate_scaler_source=train_set)\n",
    "    return train_set, validation_set\n",
    "\n",
    "def create_dataloaders(train_set, validation_set):\n",
    "    train_loader = DataLoader(train_set, batch_size=train_set.batch_size, shuffle=False, drop_last=True)\n",
    "    validation_loader = DataLoader(validation_set, batch_size=train_set.batch_size, shuffle=False, drop_last=True)\n",
    "    return train_loader, validation_loader\n",
    "\n",
    "def create_unbatched_loader(dataset: TaxiDataset):\n",
    "    dataset.set_max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = split_taxi_data_on_timestamp(zero_taxi_df, split_timestamp)\n",
    "train_set, validation_set = create_datasets(train_df, validation_df, sequence_length, continuous_features=True, time_features=True)\n",
    "train_loader, validation_loader = create_dataloaders(train_set, validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = zero_taxi_df.copy()\n",
    "test['pu_hour'] = test['pickup_datetime'].dt.hour\n",
    "test['pu_dayofweek'] = test['pickup_datetime'].dt.dayofweek\n",
    "test['pu_month'] = test['pickup_datetime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>counts</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>pu_hour</th>\n",
       "      <th>pu_dayofweek</th>\n",
       "      <th>pu_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-01 00:00:00-05:00</td>\n",
       "      <td>8</td>\n",
       "      <td>28.588137</td>\n",
       "      <td>2.320410</td>\n",
       "      <td>20.518707</td>\n",
       "      <td>3.363750</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>14.481250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-01 01:00:00-05:00</td>\n",
       "      <td>9</td>\n",
       "      <td>28.869495</td>\n",
       "      <td>3.620874</td>\n",
       "      <td>19.499601</td>\n",
       "      <td>3.298889</td>\n",
       "      <td>1.888889</td>\n",
       "      <td>13.685184</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-01 02:00:00-05:00</td>\n",
       "      <td>22</td>\n",
       "      <td>23.340609</td>\n",
       "      <td>3.562880</td>\n",
       "      <td>14.028709</td>\n",
       "      <td>2.087273</td>\n",
       "      <td>1.772727</td>\n",
       "      <td>9.458334</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-01 03:00:00-05:00</td>\n",
       "      <td>5</td>\n",
       "      <td>24.288097</td>\n",
       "      <td>2.502337</td>\n",
       "      <td>16.036741</td>\n",
       "      <td>2.504000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>11.223333</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-01 04:00:00-05:00</td>\n",
       "      <td>5</td>\n",
       "      <td>25.474213</td>\n",
       "      <td>0.756450</td>\n",
       "      <td>19.120031</td>\n",
       "      <td>2.632000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>11.460000</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236248</th>\n",
       "      <td>263</td>\n",
       "      <td>2024-03-31 19:00:00-04:00</td>\n",
       "      <td>78</td>\n",
       "      <td>16.183334</td>\n",
       "      <td>2.029487</td>\n",
       "      <td>10.134615</td>\n",
       "      <td>1.624231</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>7.688675</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236311</th>\n",
       "      <td>263</td>\n",
       "      <td>2024-03-31 20:00:00-04:00</td>\n",
       "      <td>70</td>\n",
       "      <td>18.672571</td>\n",
       "      <td>2.578286</td>\n",
       "      <td>11.130000</td>\n",
       "      <td>1.879571</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>8.332857</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236374</th>\n",
       "      <td>263</td>\n",
       "      <td>2024-03-31 21:00:00-04:00</td>\n",
       "      <td>61</td>\n",
       "      <td>19.478851</td>\n",
       "      <td>2.141803</td>\n",
       "      <td>12.394426</td>\n",
       "      <td>1.955574</td>\n",
       "      <td>1.098361</td>\n",
       "      <td>8.524590</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236437</th>\n",
       "      <td>263</td>\n",
       "      <td>2024-03-31 22:00:00-04:00</td>\n",
       "      <td>26</td>\n",
       "      <td>19.896152</td>\n",
       "      <td>2.365385</td>\n",
       "      <td>12.530768</td>\n",
       "      <td>2.263846</td>\n",
       "      <td>1.153846</td>\n",
       "      <td>8.896795</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236500</th>\n",
       "      <td>263</td>\n",
       "      <td>2024-03-31 23:00:00-04:00</td>\n",
       "      <td>21</td>\n",
       "      <td>17.670952</td>\n",
       "      <td>2.428095</td>\n",
       "      <td>10.433333</td>\n",
       "      <td>1.850952</td>\n",
       "      <td>1.095238</td>\n",
       "      <td>6.676984</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1236501 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PULocationID           pickup_datetime  counts  total_amount  \\\n",
       "0                   4 2022-01-01 00:00:00-05:00       8     28.588137   \n",
       "63                  4 2022-01-01 01:00:00-05:00       9     28.869495   \n",
       "126                 4 2022-01-01 02:00:00-05:00      22     23.340609   \n",
       "189                 4 2022-01-01 03:00:00-05:00       5     24.288097   \n",
       "252                 4 2022-01-01 04:00:00-05:00       5     25.474213   \n",
       "...               ...                       ...     ...           ...   \n",
       "1236248           263 2024-03-31 19:00:00-04:00      78     16.183334   \n",
       "1236311           263 2024-03-31 20:00:00-04:00      70     18.672571   \n",
       "1236374           263 2024-03-31 21:00:00-04:00      61     19.478851   \n",
       "1236437           263 2024-03-31 22:00:00-04:00      26     19.896152   \n",
       "1236500           263 2024-03-31 23:00:00-04:00      21     17.670952   \n",
       "\n",
       "         tip_amount  fare_amount  trip_distance  passenger_count  \\\n",
       "0          2.320410    20.518707       3.363750         1.375000   \n",
       "63         3.620874    19.499601       3.298889         1.888889   \n",
       "126        3.562880    14.028709       2.087273         1.772727   \n",
       "189        2.502337    16.036741       2.504000         1.600000   \n",
       "252        0.756450    19.120031       2.632000         1.200000   \n",
       "...             ...          ...            ...              ...   \n",
       "1236248    2.029487    10.134615       1.624231         1.333333   \n",
       "1236311    2.578286    11.130000       1.879571         1.428571   \n",
       "1236374    2.141803    12.394426       1.955574         1.098361   \n",
       "1236437    2.365385    12.530768       2.263846         1.153846   \n",
       "1236500    2.428095    10.433333       1.850952         1.095238   \n",
       "\n",
       "         trip_duration  pu_hour  pu_dayofweek  pu_month  \n",
       "0            14.481250        0             5         1  \n",
       "63           13.685184        1             5         1  \n",
       "126           9.458334        2             5         1  \n",
       "189          11.223333        3             5         1  \n",
       "252          11.460000        4             5         1  \n",
       "...                ...      ...           ...       ...  \n",
       "1236248       7.688675       19             6         3  \n",
       "1236311       8.332857       20             6         3  \n",
       "1236374       8.524590       21             6         3  \n",
       "1236437       8.896795       22             6         3  \n",
       "1236500       6.676984       23             6         3  \n",
       "\n",
       "[1236501 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sort_values([\"PULocationID\", \"pickup_datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelType(Enum):\n",
    "    RNN = 1\n",
    "    LSTM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "class BaselineModel(nn.Module):\n",
    "    savable = False\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.fc = nn.Linear(3, 10)\n",
    "    \n",
    "    def forward(self, _, continuous: torch.tensor):\n",
    "        constant_pred = continuous[:, :, 0].detach().clone()\n",
    "        constant_pred.requires_grad_()\n",
    "        return constant_pred\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSeriesModel(nn.Module):\n",
    "    savable = True\n",
    "    def __init__(\n",
    "            self, \n",
    "            zone_count, \n",
    "            model_type: ModelType, \n",
    "            batch_size: int, \n",
    "            input_size=1, \n",
    "            hidden_size=50, \n",
    "            output_size=1, \n",
    "            num_layers=1\n",
    "        ):\n",
    "        super(MultiSeriesModel, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.embed_size = embed_size(zone_count)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.zone_embed = nn.Embedding(zone_count, self.embed_size)\n",
    "        \n",
    "        if self.model_type == ModelType.RNN:\n",
    "            self.cell = nn.RNN(self.embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.h = torch.zeros(num_layers, batch_size, hidden_size, device=device)\n",
    "        elif self.model_type == ModelType.LSTM:\n",
    "            self.cell = nn.LSTM(self.embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.h = [torch.zeros(num_layers, batch_size, hidden_size, device=device) for _ in range(2)]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model only supports RNN and LSTM\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, zones, continuous):\n",
    "        # Zones is (zone_count, 1), emb (zone_count, embed_size(zone_count))\n",
    "        sequence_length = continuous.shape[-2]\n",
    "        embed_result = self.zone_embed(zones).expand((-1, sequence_length, -1))\n",
    "        x = torch.cat([embed_result, continuous], dim=-1)\n",
    "        if self.model_type == ModelType.RNN:\n",
    "            out, h = self.cell(x, self.h)\n",
    "            self.h = h.detach()\n",
    "        elif self.model_type == ModelType.LSTM:\n",
    "            out, h = self.cell(x, self.h)\n",
    "            self.h = [h_.detach() for h_ in h]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model only supports RNN and LSTM\")\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out.squeeze()\n",
    "\n",
    "    def reset(self):\n",
    "        if type(self.h) is list:\n",
    "            for h in self.h: \n",
    "                h.zero_()\n",
    "        else:\n",
    "            self.h.zero_()\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        if self.input_size > 1:\n",
    "            return f\"{self.model_type.name}_MultiSeriesModel\"\n",
    "        else:\n",
    "            return f\"{self.model_type.name}_SingleSeriesModel\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "class MultiSeriesTimeModel(nn.Module):\n",
    "    savable = True\n",
    "    def __init__(\n",
    "            self, \n",
    "            zone_count,\n",
    "            int_features: Dict[int, int],  # index and number of distinct values\n",
    "            model_type: ModelType, \n",
    "            batch_size: int, \n",
    "            input_size=1, \n",
    "            hidden_size=50, \n",
    "            output_size=1, \n",
    "            num_layers=1\n",
    "        ):\n",
    "        super(MultiSeriesTimeModel, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.embed_size = embed_size(zone_count)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.zone_embed = nn.Embedding(zone_count, self.embed_size)\n",
    "\n",
    "        self.int_embeds = {}\n",
    "\n",
    "        int_embed_size = 0\n",
    "        for idx, num_unique in int_features.items():\n",
    "            self.int_embeds[idx] = nn.Embedding(num_unique, embed_size(num_unique)).to(device)\n",
    "            int_embed_size += embed_size(num_unique)\n",
    "\n",
    "\n",
    "        recurrent_size = self.embed_size + input_size + int_embed_size\n",
    "        if self.model_type == ModelType.RNN:\n",
    "            self.cell = nn.RNN(recurrent_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.h = torch.zeros(num_layers, batch_size, hidden_size, device=device)\n",
    "        elif self.model_type == ModelType.LSTM:\n",
    "            self.cell = nn.LSTM(recurrent_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.h = [torch.zeros(num_layers, batch_size, hidden_size, device=device) for _ in range(2)]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model only supports RNN and LSTM\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, zones, continuous, int_features):\n",
    "        # Zones is (zone_count, 1), emb (zone_count, embed_size(zone_count))\n",
    "        sequence_length = continuous.shape[-2]\n",
    "        embed_result = self.zone_embed(zones).expand((-1, sequence_length, -1))\n",
    "\n",
    "        int_embed_results = {}\n",
    "        for idx, embed in self.int_embeds.items():\n",
    "            int_embed_results[idx] = embed(int_features[:, :, idx])\n",
    "\n",
    "        embed_result_arr = []\n",
    "        for idx in sorted(list(int_embed_results.keys())):\n",
    "            embed_result_arr.append(int_embed_results[idx])\n",
    "\n",
    "        x = torch.cat(embed_result_arr + [embed_result, continuous], dim=-1)\n",
    "        if self.model_type == ModelType.RNN:\n",
    "            out, h = self.cell(x, self.h)\n",
    "            self.h = h.detach()\n",
    "        elif self.model_type == ModelType.LSTM:\n",
    "            out, h = self.cell(x, self.h)\n",
    "            self.h = [h_.detach() for h_ in h]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model only supports RNN and LSTM\")\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out.squeeze()\n",
    "\n",
    "    def reset(self):\n",
    "        if type(self.h) is list:\n",
    "            for h in self.h: \n",
    "                h.zero_()\n",
    "        else:\n",
    "            self.h.zero_()\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        if self.input_size > 1:\n",
    "            return f\"{self.model_type.name}_MultiSeriesTimeModel\"\n",
    "        else:\n",
    "            return f\"{self.model_type.name}_SingleSeriesTimeModel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = BaselineModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple and time series variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLoop:\n",
    "    NUM_EPOCHS = 1000\n",
    "    MODEL_FOLDER = \"../models/model_validation/\"\n",
    "    PRINT_EVERY = 10\n",
    "\n",
    "    def __init__(self, model, train_loader, validation_loader):\n",
    "        self.model: nn.Module = model\n",
    "        self.optimizer: optim.Optimizer = optim.Adam(model.parameters(), lr=3e-3)\n",
    "        self.criterion: nn.MSELoss = nn.MSELoss()\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "        self.min_valid_loss = float(\"inf\")\n",
    "        self.best_model_path = \"\"\n",
    "\n",
    "        self.valid_loss_list = []\n",
    "        self.consecutive_loss_increases = 0\n",
    "\n",
    "\n",
    "    def train_epoch(self) -> float:\n",
    "        self.model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for *vars, target in self.train_loader:\n",
    "            preds = self.model(*vars)\n",
    "            train_loss = self.criterion(preds, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_train_loss += train_loss.item()\n",
    "        self.model.reset()\n",
    "\n",
    "        avg_train_loss = epoch_train_loss / len(self.train_loader)\n",
    "        return avg_train_loss\n",
    "\n",
    "    def validate_epoch(self) -> float:\n",
    "        self.model.eval()\n",
    "        epoch_valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for *vars, target in self.validation_loader:\n",
    "                preds = self.model(*vars)            \n",
    "                valid_loss = self.criterion(preds, target)\n",
    "\n",
    "                epoch_valid_loss += valid_loss.item()\n",
    "        self.model.reset()\n",
    "        \n",
    "        avg_valid_loss = epoch_valid_loss / len(self.validation_loader)\n",
    "        return avg_valid_loss\n",
    "    \n",
    "    def print_loss(self, epoch, train_loss, valid_loss):\n",
    "        print(f'Epoch [{epoch + 1:04}/{self.NUM_EPOCHS}], Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "    \n",
    "    def fit_one_epoch(self):\n",
    "        avg_train_loss = self.train_epoch()\n",
    "        avg_valid_loss = self.validate_epoch()\n",
    "        return avg_train_loss, avg_valid_loss\n",
    "    \n",
    "    def save_model(self, epoch) -> str:\n",
    "        if self.model.savable:\n",
    "            model_name = f\"{self.model.get_model_name()}_{self.timestamp}_{epoch}\"\n",
    "            model_path = self.MODEL_FOLDER + model_name\n",
    "            torch.save(self.model.state_dict(), model_path)\n",
    "            return model_path\n",
    "\n",
    "    def handle_early_stopping(self, epoch) -> bool:\n",
    "        stop = False\n",
    "        if self.valid_loss_list[-1] < self.min_valid_loss:\n",
    "            # Save model\n",
    "            self.min_valid_loss = self.valid_loss_list[-1]\n",
    "            self.best_model_path = self.save_model(epoch)\n",
    "            # Reset count\n",
    "            self.consecutive_loss_increases = 0\n",
    "        else:\n",
    "            self.consecutive_loss_increases += 1\n",
    "            if self.consecutive_loss_increases >= 20:\n",
    "                stop = True\n",
    "\n",
    "        return stop\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.NUM_EPOCHS):\n",
    "            avg_train_loss, avg_valid_loss = self.fit_one_epoch()\n",
    "\n",
    "            if (epoch + 1) % self.PRINT_EVERY == 0:\n",
    "                self.print_loss(epoch, avg_train_loss, avg_valid_loss)\n",
    "\n",
    "            ## Early Stopping: check if the error went up or down\n",
    "            self.valid_loss_list.append(avg_valid_loss)\n",
    "            stop = self.handle_early_stopping(epoch)\n",
    "            if stop or not self.model.savable:\n",
    "                break\n",
    "        if self.model.savable:\n",
    "            self.model.load_state_dict(torch.load(self.best_model_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better than baseline !!!\n",
    "# 0.08\n",
    "# np.mean(np.power(np.array(train_set.continuous_features.cpu()[:-1, :] - train_set.continuous_features.cpu()[1:, :]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            train_set: TaxiDataset,\n",
    "            validation_set: TaxiDataset\n",
    "        ) -> None:\n",
    "        validation_set.set_max_sequence_length()\n",
    "        self.train_set = train_set\n",
    "        self.validation_set = validation_set\n",
    "        self.validation_loader = DataLoader(validation_set, validation_set.batch_size, shuffle=False)\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "        # TODO: Validation set needs to be scaled by train set scalers\n",
    "        \n",
    "    def get_preds(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for *vars, target in self.validation_loader:\n",
    "                preds = self.model(*vars)\n",
    "        self.model.reset()\n",
    "        # Transpose so that time is along vertical\n",
    "        preds = preds.cpu().detach().numpy().T\n",
    "        target = target.cpu().detach().numpy().T\n",
    "        \n",
    "        return preds, target\n",
    "    \n",
    "    def unscale_preds_and_target(self, scaled_preds, scaled_target):\n",
    "        preds = self.train_set.count_scaler.inverse_transform(scaled_preds)\n",
    "        target = self.validation_set.count_scaler.inverse_transform(scaled_target)\n",
    "        return preds, target\n",
    "\n",
    "    def mse_loss(self, preds, target):\n",
    "        # mse_loss = nn.MSELoss()\n",
    "        # scaled_loss = mse_loss(preds, target)\n",
    "        # print(type(scaled_loss))\n",
    "        # return scaled_loss.item()\n",
    "        return ((preds - target) ** 2).mean()\n",
    "\n",
    "    def smape_loss(self, preds, target):\n",
    "        # TODO: Eliminiate runtime warning when denominator == 0\n",
    "        smape_mat = (2 * np.abs(preds - target)) / (np.abs(preds) + np.abs(target))\n",
    "        smape_mat = np.mean(np.nan_to_num(smape_mat))\n",
    "        return smape_mat\n",
    "\n",
    "    def validate(self):\n",
    "        scaled_preds, scaled_target = self.get_preds()\n",
    "        preds, target = self.unscale_preds_and_target(scaled_preds, scaled_target)\n",
    "        \n",
    "        self.preds = preds\n",
    "        self.target = target\n",
    "        self.scaled_mse_loss = self.mse_loss(scaled_preds, scaled_target)\n",
    "        self.unscaled_mse_loss = self.mse_loss(preds, target)\n",
    "        self.smape_loss = self.smape_loss(preds, target)\n",
    "\n",
    "    def print_results(self):\n",
    "        print(f\"Scaled MSE: {self.scaled_mse_loss:.4f}, Unscaled MSE: {self.unscaled_mse_loss}, sMAPE: {self.smape_loss}\")\n",
    "    \n",
    "    # TODO: Some kind of plot support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelContainer:\n",
    "    # Act as a container for all of the other models\n",
    "    def __init__(self, taxi_df, model_type, sequence_length, continuous_features, time_features) -> None:\n",
    "        # Configuration options\n",
    "        self.continuous_features = continuous_features\n",
    "        self.time_features = time_features\n",
    "\n",
    "        self.data = taxi_df\n",
    "        self.model_type = model_type\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Setup\n",
    "        self.create_datasets()\n",
    "        self.setup_model()\n",
    "\n",
    "    def run(self):\n",
    "        self.train_model()\n",
    "        self.validate_model()\n",
    "\n",
    "    def create_datasets(self):\n",
    "        train_df, validation_df = split_taxi_data_on_timestamp(self.data, split_timestamp)\n",
    "        train_set, validation_set = create_datasets(\n",
    "            train_df, validation_df, self.sequence_length, \n",
    "            self.continuous_features, self.time_features)\n",
    "\n",
    "        self.train_set = train_set\n",
    "\n",
    "        train_loader, validation_loader = create_dataloaders(train_set, validation_set)\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "\n",
    "        self.validation_set_for_validation = TaxiDataset(validation_df, self.sequence_length, self.continuous_features, self.time_features)\n",
    "\n",
    "    def setup_model(self):\n",
    "        if self.model_type == 'rnn':\n",
    "            model_type = ModelType.RNN\n",
    "        elif self.model_type == 'lstm':\n",
    "            model_type = ModelType.LSTM\n",
    "        else:\n",
    "            raise ValueError(f\"mode_type {self.model_type} not supported\")\n",
    "        \n",
    "\n",
    "        if self.model_type == 'baseline':\n",
    "            self.model = BaselineModel()\n",
    "        elif self.time_features:\n",
    "            self.model = MultiSeriesTimeModel(train_set.num_zones, train_set.int_features_unique,  model_type, train_set.batch_size, self.train_set.num_continuous_features).to(device)\n",
    "        else:\n",
    "            self.model = MultiSeriesModel(train_set.num_zones, model_type, train_set.batch_size, self.train_set.num_continuous_features).to(device)\n",
    "\n",
    "    def train_model(self):\n",
    "        loop = TrainingLoop(self.model, self.train_loader, self.validation_loader)\n",
    "        loop.train()\n",
    "        \n",
    "    def validate_model(self):\n",
    "        valid = Validator(self.model, self.train_set, self.validation_set_for_validation)\n",
    "        valid.validate()\n",
    "        valid.print_results()\n",
    "    \n",
    "    # Dataset generation\n",
    "    # Create the Model\n",
    "    # Fit the model\n",
    "    # Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0057, Validation Loss: 0.0103\n",
      "Epoch [0020/1000], Train Loss: 0.0051, Validation Loss: 0.0057\n",
      "Epoch [0030/1000], Train Loss: 0.0042, Validation Loss: 0.0056\n",
      "Epoch [0040/1000], Train Loss: 0.0042, Validation Loss: 0.0059\n",
      "Epoch [0050/1000], Train Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch [0060/1000], Train Loss: 0.0038, Validation Loss: 0.0044\n",
      "Epoch [0070/1000], Train Loss: 0.0037, Validation Loss: 0.0042\n",
      "Epoch [0080/1000], Train Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch [0090/1000], Train Loss: 0.0036, Validation Loss: 0.0040\n",
      "Epoch [0100/1000], Train Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch [0110/1000], Train Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch [0120/1000], Train Loss: 0.0035, Validation Loss: 0.0039\n",
      "Epoch [0130/1000], Train Loss: 0.0035, Validation Loss: 0.0039\n",
      "Epoch [0140/1000], Train Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch [0150/1000], Train Loss: 0.0034, Validation Loss: 0.0040\n",
      "Scaled MSE: 0.0051, Unscaled MSE: 271.08770751953125, sMAPE: 0.5536814332008362\n"
     ]
    }
   ],
   "source": [
    "# Varies by 10\n",
    "test = ModelContainer(mean_taxi_df, 'lstm', 96, True, True)\n",
    "test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0050, Validation Loss: 0.0050\n",
      "Epoch [0020/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0030/1000], Train Loss: 0.0045, Validation Loss: 0.0045\n",
      "Epoch [0040/1000], Train Loss: 0.0044, Validation Loss: 0.0044\n",
      "Epoch [0050/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0060/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0070/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0080/1000], Train Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch [0090/1000], Train Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch [0100/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0110/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0120/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0130/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0140/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch [0150/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch [0160/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch [0170/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch [0180/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Scaled MSE: 0.0059, Unscaled MSE: 374.28741455078125, sMAPE: 0.5773265361785889\n"
     ]
    }
   ],
   "source": [
    "test = ModelContainer(mean_taxi_df, 'lstm', 96, True, False)\n",
    "test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0053, Validation Loss: 0.0075\n",
      "Epoch [0020/1000], Train Loss: 0.0044, Validation Loss: 0.0051\n",
      "Epoch [0030/1000], Train Loss: 0.0040, Validation Loss: 0.0046\n",
      "Epoch [0040/1000], Train Loss: 0.0039, Validation Loss: 0.0046\n",
      "Epoch [0050/1000], Train Loss: 0.0038, Validation Loss: 0.0044\n",
      "Epoch [0060/1000], Train Loss: 0.0037, Validation Loss: 0.0043\n",
      "Epoch [0070/1000], Train Loss: 0.0036, Validation Loss: 0.0043\n",
      "Epoch [0080/1000], Train Loss: 0.0036, Validation Loss: 0.0042\n",
      "Epoch [0090/1000], Train Loss: 0.0036, Validation Loss: 0.0042\n",
      "Epoch [0100/1000], Train Loss: 0.0035, Validation Loss: 0.0041\n",
      "Epoch [0110/1000], Train Loss: 0.0035, Validation Loss: 0.0041\n",
      "Epoch [0120/1000], Train Loss: 0.0034, Validation Loss: 0.0041\n",
      "Epoch [0130/1000], Train Loss: 0.0034, Validation Loss: 0.0041\n",
      "Epoch [0140/1000], Train Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch [0150/1000], Train Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch [0160/1000], Train Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch [0170/1000], Train Loss: 0.0033, Validation Loss: 0.0040\n",
      "Epoch [0180/1000], Train Loss: 0.0033, Validation Loss: 0.0040\n",
      "Epoch [0190/1000], Train Loss: 0.0033, Validation Loss: 0.0040\n",
      "Scaled MSE: 0.0049, Unscaled MSE: 251.4924774169922, sMAPE: 0.5780544281005859\n"
     ]
    }
   ],
   "source": [
    "test = ModelContainer(mean_taxi_df, 'lstm', 96, False, True)\n",
    "test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0050, Validation Loss: 0.0049\n",
      "Epoch [0020/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0030/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0040/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0050/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0060/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0070/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0080/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0090/1000], Train Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch [0100/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0110/1000], Train Loss: 0.0039, Validation Loss: 0.0042\n",
      "Scaled MSE: 0.0052, Unscaled MSE: 274.9544982910156, sMAPE: 0.5505231618881226\n"
     ]
    }
   ],
   "source": [
    "test = ModelContainer(mean_taxi_df, 'lstm', 96, False, False)\n",
    "test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0076, Validation Loss: 0.0075\n",
      "Epoch [0020/1000], Train Loss: 0.0067, Validation Loss: 0.0066\n",
      "Epoch [0030/1000], Train Loss: 0.0057, Validation Loss: 0.0056\n",
      "Epoch [0040/1000], Train Loss: 0.0054, Validation Loss: 0.0054\n",
      "Epoch [0050/1000], Train Loss: 0.0052, Validation Loss: 0.0052\n",
      "Epoch [0060/1000], Train Loss: 0.0052, Validation Loss: 0.0051\n",
      "Epoch [0070/1000], Train Loss: 0.0050, Validation Loss: 0.0050\n",
      "Epoch [0080/1000], Train Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch [0090/1000], Train Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch [0100/1000], Train Loss: 0.0050, Validation Loss: 0.0049\n",
      "Epoch [0110/1000], Train Loss: 0.0048, Validation Loss: 0.0049\n",
      "Epoch [0120/1000], Train Loss: 0.0048, Validation Loss: 0.0048\n",
      "Epoch [0130/1000], Train Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch [0140/1000], Train Loss: 0.0047, Validation Loss: 0.0047\n",
      "Epoch [0150/1000], Train Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch [0160/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0170/1000], Train Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch [0180/1000], Train Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch [0190/1000], Train Loss: 0.0045, Validation Loss: 0.0045\n",
      "Epoch [0200/1000], Train Loss: 0.0045, Validation Loss: 0.0045\n",
      "Epoch [0210/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0220/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0230/1000], Train Loss: 0.0044, Validation Loss: 0.0044\n",
      "Epoch [0240/1000], Train Loss: 0.0044, Validation Loss: 0.0044\n",
      "Epoch [0250/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0260/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0270/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0280/1000], Train Loss: 0.0043, Validation Loss: 0.0043\n",
      "Epoch [0290/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0300/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0310/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0320/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0330/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch [0340/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0350/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch [0360/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0370/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0380/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0390/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0400/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0410/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0420/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0430/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0440/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0450/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0460/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Scaled MSE: 0.0058, Unscaled MSE: 371.2231750488281, sMAPE: 0.6074097752571106\n"
     ]
    }
   ],
   "source": [
    "test1 = ModelContainer(mean_taxi_df, 'lstm', 1000)\n",
    "test1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0075, Validation Loss: 0.0074\n",
      "Epoch [0020/1000], Train Loss: 0.0062, Validation Loss: 0.0060\n",
      "Epoch [0030/1000], Train Loss: 0.0056, Validation Loss: 0.0055\n",
      "Epoch [0040/1000], Train Loss: 0.0053, Validation Loss: 0.0053\n",
      "Epoch [0050/1000], Train Loss: 0.0052, Validation Loss: 0.0052\n",
      "Epoch [0060/1000], Train Loss: 0.0051, Validation Loss: 0.0051\n",
      "Epoch [0070/1000], Train Loss: 0.0050, Validation Loss: 0.0050\n",
      "Epoch [0080/1000], Train Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch [0090/1000], Train Loss: 0.0049, Validation Loss: 0.0049\n",
      "Epoch [0100/1000], Train Loss: 0.0048, Validation Loss: 0.0049\n",
      "Epoch [0110/1000], Train Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch [0120/1000], Train Loss: 0.0047, Validation Loss: 0.0048\n",
      "Epoch [0130/1000], Train Loss: 0.0047, Validation Loss: 0.0047\n",
      "Epoch [0140/1000], Train Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch [0150/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0160/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0170/1000], Train Loss: 0.0046, Validation Loss: 0.0047\n",
      "Epoch [0180/1000], Train Loss: 0.0047, Validation Loss: 0.0046\n",
      "Epoch [0190/1000], Train Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch [0200/1000], Train Loss: 0.0045, Validation Loss: 0.0045\n",
      "Epoch [0210/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0220/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0230/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0240/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0250/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0260/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0270/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0280/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0290/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0300/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0310/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0320/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0330/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch [0340/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch [0350/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0360/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Scaled MSE: 0.0061, Unscaled MSE: 391.5155944824219, sMAPE: 0.6256916522979736\n"
     ]
    }
   ],
   "source": [
    "test2 = ModelContainer(zero_taxi_df, 'lstm', 1000)\n",
    "test2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_and_validation_sets(taxi_df, split_timestamp):\n",
    "    train_df, validation_df = split_taxi_data_on_timestamp(taxi_df, split_timestamp)\n",
    "    train_set, validation_set = create_datasets(train_df, validation_df, sequence_length, continuous_features=True)\n",
    "    return train_set, validation_set\n",
    "\n",
    "def setup_models(train_set: TaxiDataset):\n",
    "    baseline = BaselineModel()\n",
    "    rnn = MultiSeriesModel(train_set.num_zones, ModelType.RNN, train_set.batch_size, train_set.num_continuous_features).to(device)\n",
    "    lstm = MultiSeriesModel(train_set.num_zones, ModelType.LSTM, train_set.batch_size, train_set.num_continuous_features).to(device)\n",
    "    models = [baseline, rnn, lstm]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(train_set):\n",
    "    models = setup_models(train_set)\n",
    "    return models\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, validation_loader):\n",
    "    # The models should have the best parameters by the end\n",
    "    loop = TrainingLoop(model, train_loader, validation_loader)\n",
    "    loop.train()\n",
    "    return loop\n",
    "\n",
    "\n",
    "def validate_models(model, train_set, taxi_df, split_timestamp, continuous_features):\n",
    "    _, validation_df = split_taxi_data_on_timestamp(taxi_df, split_timestamp)\n",
    "    validation_set = TaxiDataset(validation_df, sequence_length, continuous_features)\n",
    "    valid = Validator(model, train_set, validation_set)\n",
    "    valid.validate()\n",
    "    valid.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, validation_set = create_train_and_validation_sets(mean_taxi_df, split_timestamp)\n",
    "train_loader, validation_loader = create_dataloaders(train_set, validation_set)\n",
    "models = create_models(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = BaselineModel().to(device)\n",
    "rnn = MultiSeriesModel(train_set.num_zones, ModelType.RNN, train_set.batch_size, train_set.num_continuous_features).to(device)\n",
    "lstm = MultiSeriesModel(train_set.num_zones, ModelType.LSTM, train_set.batch_size, train_set.num_continuous_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_single.num_continuous_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = split_taxi_data_on_timestamp(mean_taxi_df, split_timestamp)\n",
    "train_set_single, validation_set_single = create_datasets(train_df, validation_df, sequence_length)\n",
    "train_loader_single, validation_loader_single = create_dataloaders(train_set_single, validation_set_single)\n",
    "lstm_single = MultiSeriesModel(train_set.num_zones, ModelType.LSTM, train_set_single.batch_size, train_set_single.num_continuous_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0049, Validation Loss: 0.0049\n",
      "Epoch [0020/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0030/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0040/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0050/1000], Train Loss: 0.0041, Validation Loss: 0.0043\n",
      "Epoch [0060/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0070/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0080/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0090/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0100/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0110/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0120/1000], Train Loss: 0.0039, Validation Loss: 0.0042\n",
      "Epoch [0130/1000], Train Loss: 0.0038, Validation Loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "lstm_single_loop = train_model(lstm_single, train_loader_single, validation_loader_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_loop = train_model(baseline, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_models(baseline, train_set_single, mean_taxi_df, split_timestamp, continuous_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0057, Validation Loss: 0.0058\n",
      "Epoch [0020/1000], Train Loss: 0.0052, Validation Loss: 0.0054\n",
      "Epoch [0030/1000], Train Loss: 0.0049, Validation Loss: 0.0050\n",
      "Epoch [0040/1000], Train Loss: 0.0047, Validation Loss: 0.0049\n",
      "Epoch [0050/1000], Train Loss: 0.0046, Validation Loss: 0.0049\n",
      "Epoch [0060/1000], Train Loss: 0.0046, Validation Loss: 0.0048\n",
      "Epoch [0070/1000], Train Loss: 0.0045, Validation Loss: 0.0048\n",
      "Epoch [0080/1000], Train Loss: 0.0045, Validation Loss: 0.0047\n",
      "Epoch [0090/1000], Train Loss: 0.0044, Validation Loss: 0.0047\n",
      "Epoch [0100/1000], Train Loss: 0.0044, Validation Loss: 0.0046\n",
      "Epoch [0110/1000], Train Loss: 0.0044, Validation Loss: 0.0048\n",
      "Epoch [0120/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0130/1000], Train Loss: 0.0044, Validation Loss: 0.0046\n",
      "Epoch [0140/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0150/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0160/1000], Train Loss: 0.0045, Validation Loss: 0.0046\n",
      "Epoch [0170/1000], Train Loss: 0.0044, Validation Loss: 0.0046\n"
     ]
    }
   ],
   "source": [
    "rnn_loop = train_model(rnn, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0049, Validation Loss: 0.0049\n",
      "Epoch [0020/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0030/1000], Train Loss: 0.0044, Validation Loss: 0.0044\n",
      "Epoch [0040/1000], Train Loss: 0.0043, Validation Loss: 0.0043\n",
      "Epoch [0050/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0060/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0070/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0080/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0090/1000], Train Loss: 0.0039, Validation Loss: 0.0042\n",
      "Epoch [0100/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0110/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0120/1000], Train Loss: 0.0038, Validation Loss: 0.0041\n",
      "Epoch [0130/1000], Train Loss: 0.0038, Validation Loss: 0.0041\n",
      "Epoch [0140/1000], Train Loss: 0.0038, Validation Loss: 0.0041\n",
      "Epoch [0150/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch [0160/1000], Train Loss: 0.0038, Validation Loss: 0.0041\n",
      "Epoch [0170/1000], Train Loss: 0.0037, Validation Loss: 0.0040\n"
     ]
    }
   ],
   "source": [
    "lstm_loop = train_model(lstm, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_loop = train_model(lstm, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled MSE: 0.0051, Unscaled MSE: 274.3553771972656, sMAPE: 0.5782305598258972\n"
     ]
    }
   ],
   "source": [
    "validate_models(lstm_single, train_set_single, mean_taxi_df, split_timestamp, continuous_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled MSE: 0.0063, Unscaled MSE: 390.16009521484375, sMAPE: 0.6303727626800537\n"
     ]
    }
   ],
   "source": [
    "validate_models(rnn, train_set, mean_taxi_df, split_timestamp, continuous_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled MSE: 0.0058, Unscaled MSE: 342.6379699707031, sMAPE: 0.5724579691886902\n"
     ]
    }
   ],
   "source": [
    "validate_models(lstm, train_set, mean_taxi_df, split_timestamp, continuous_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_set = TaxiDataset(validation_df, sequence_length, continuous_features)\n",
    "valid = Validator(model, train_set, validation_set)\n",
    "valid.validate()\n",
    "valid.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
