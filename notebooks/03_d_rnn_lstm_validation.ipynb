{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..') # add parent directory to path\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from utils import processing as pr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_size(distinct_values: int):\n",
    "    return min(50, (distinct_values + 1) // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_taxi_df = pd.read_pickle('data/test/adjusted_yellow_2022-01_2024-03_bypulocation.pkl')\n",
    "zero_taxi_df = pd.read_pickle('data/test/yellow_2022-01_2024-03_bypulocation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_taxi_df.to_csv(\"./taxi_actual.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset implementation\n",
    "import torch.utils\n",
    "\n",
    "# Datset paramaters\n",
    "# TODO: put these somewhere better\n",
    "batch_size = None  # Equal to the number of zones for convenience\n",
    "sequence_length = 96\n",
    "\n",
    "HOURS_PER_SERIES = 19627\n",
    "\n",
    "\n",
    "class TaxiDataset(torch.utils.data.Dataset):\n",
    "    # The dataset must have batch_size == num_zones\n",
    "    def __init__(\n",
    "            self, \n",
    "            taxi_data: pd.DataFrame, \n",
    "            sequence_length, \n",
    "            continuous_features=False, \n",
    "            time_features=False, \n",
    "            use_alternate_scaler=False, \n",
    "            alternate_scaler_source=None\n",
    "        ):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_zones = taxi_data[\"PULocationID\"].nunique()\n",
    "        self.zones = torch.tensor(list(range(self.num_zones))).to(device)\n",
    "        self.num_hours = taxi_data[\"pickup_datetime\"].nunique()\n",
    "        \n",
    "        # Simplest way to organize the data is to have all zones in a batch\n",
    "        self.batch_size = self.num_zones\n",
    "        self.num_continuous_features = 1\n",
    "        self.num_integer_features = 0\n",
    "\n",
    "        self.use_alternate_scaler = use_alternate_scaler \n",
    "        # self.alternate_scaler_source=alternate_scaler_source\n",
    "\n",
    "        taxi_data[\"counts\"] = taxi_data[\"counts\"].astype(np.float32)\n",
    "\n",
    "        # All of the preprocessing will happpen here\n",
    "        features_to_keep = [\"PULocationID\", \"pickup_datetime\", \"counts\"]\n",
    "        if continuous_features:\n",
    "            # Add continuous feature to keep list\n",
    "            continuous_features = ['tip_amount', 'fare_amount', 'trip_distance', 'trip_duration']\n",
    "            self.num_continuous_features += len(continuous_features)\n",
    "            features_to_keep.extend(continuous_features)\n",
    "        \n",
    "        if time_features:\n",
    "            # Create time features and add them to the keep list\n",
    "            # TODO add time features to dataset\n",
    "            taxi_data = self.add_time_features(taxi_data)\n",
    "            time_features = ['pu_hour', 'pu_dayofweek', 'pu_month']\n",
    "            self.num_integer_features = len(time_features)\n",
    "            features_to_keep.extend(time_features)\n",
    "\n",
    "        # SUBSET COLUMNS\n",
    "        # Remove any columns that aren't sued for sorting or in the model\n",
    "        taxi_data = taxi_data[features_to_keep]\n",
    "        \n",
    "        # DATA SCALING\n",
    "        if self.use_alternate_scaler:\n",
    "            self.count_scaler = alternate_scaler_source.count_scaler\n",
    "            self.other_scaler = alternate_scaler_source.other_scaler\n",
    "        else:\n",
    "            self.count_scaler = MinMaxScaler()\n",
    "            self.other_scaler = MinMaxScaler()\n",
    "        \n",
    "        taxi_data = self.fit_count_scaler(taxi_data)\n",
    "        if continuous_features:\n",
    "            taxi_data = self.fit_other_scaler(taxi_data)\n",
    "    \n",
    "        # Sort to prepare for splitting\n",
    "        taxi_data = taxi_data.sort_values([\"PULocationID\", \"pickup_datetime\"], ascending=True)\n",
    "        \n",
    "        # Separate out the integer\n",
    "        if self.num_integer_features:\n",
    "            int_data = taxi_data.loc[:, time_features]\n",
    "            self.integer_features = self.features_to_tensor(int_data)\n",
    "            taxi_data = taxi_data.drop(time_features, axis=1)\n",
    "            self.int_features_unique = {}\n",
    "            for idx in range(self.num_integer_features):\n",
    "                self.int_features_unique[idx] = len(self.integer_features[:, idx].unique())\n",
    "        \n",
    "        # Drop batch_number, PULocationID, pickup_datetime\n",
    "        taxi_data = taxi_data.drop([\"PULocationID\", \"pickup_datetime\"], axis=1)\n",
    "        self.continuous_features = self.features_to_tensor(taxi_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        num_full_seqs = self.continuous_features.shape[0] // self.sequence_length\n",
    "        return self.num_zones * num_full_seqs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 0 gets zone 0 item 0\n",
    "        # 1 gets zone 1 item 0\n",
    "        # Num_zones = batch_size\n",
    "        # idx % (batch_size) = zone number\n",
    "        batch_idx, zone_idx = divmod(idx, self.batch_size)\n",
    "        cont_tensor = self.get_continuous_tensor(batch_idx, zone_idx)\n",
    "        target = self.get_target_tensor(batch_idx, zone_idx)\n",
    "        \n",
    "        zone_id_tensor = torch.tensor([zone_idx]).to(device)\n",
    "\n",
    "        if self.num_integer_features > 0:\n",
    "            int_tensor = self.get_integer_tensor(batch_idx, zone_idx)\n",
    "            return zone_id_tensor, cont_tensor, int_tensor, target\n",
    "        else:\n",
    "            return zone_id_tensor, cont_tensor, target\n",
    "\n",
    "    def set_max_sequence_length(self):\n",
    "        self.sequence_length = self.num_hours - 1\n",
    "\n",
    "    def add_time_features(self, taxi_data):\n",
    "        taxi_data['pu_hour'] = taxi_data['pickup_datetime'].dt.hour\n",
    "        taxi_data['pu_hour'] = taxi_data['pu_hour'] - np.min(taxi_data['pu_hour'])\n",
    "        taxi_data['pu_dayofweek'] = taxi_data['pickup_datetime'].dt.dayofweek\n",
    "        taxi_data['pu_dayofweek'] = taxi_data['pu_dayofweek'] - np.min(taxi_data['pu_dayofweek'])\n",
    "        taxi_data['pu_month'] = taxi_data['pickup_datetime'].dt.month\n",
    "        taxi_data['pu_month'] = taxi_data['pu_month'] - np.min(taxi_data['pu_month'])\n",
    "        return taxi_data\n",
    "\n",
    "    def get_continuous_tensor(self, batch_idx, zone_idx):\n",
    "        col_start = zone_idx * self.num_continuous_features\n",
    "        col_end = (zone_idx + 1) * self.num_continuous_features\n",
    "        row_start = batch_idx * self.sequence_length\n",
    "        row_end = (batch_idx + 1) * self.sequence_length\n",
    "        return self.continuous_features[row_start:row_end, col_start:col_end]\n",
    "    \n",
    "    def get_integer_tensor(self, batch_idx, zone_idx):\n",
    "        col_start = zone_idx * self.num_integer_features\n",
    "        col_end = (zone_idx + 1) * self.num_integer_features\n",
    "        row_start = batch_idx * self.sequence_length\n",
    "        row_end = (batch_idx + 1) * self.sequence_length\n",
    "        # print(\"Col Start: \", col_start)\n",
    "        # print(\"Col End  : \", col_end)\n",
    "        # print(\"Row Start: \", row_start)\n",
    "        # print(\"Row End  : \", row_end)\n",
    "        return self.integer_features[row_start:row_end, col_start:col_end]\n",
    "    \n",
    "    def get_target_tensor(self, batch_idx, zone_idx):\n",
    "        # TODO: Probably a lurking index out of bounds issue \n",
    "        # if data length % sequence_length == 0\n",
    "        col_idx = zone_idx * self.num_continuous_features\n",
    "        row_start = (batch_idx * self.sequence_length) + 1\n",
    "        row_end = ((batch_idx + 1) * self.sequence_length) + 1\n",
    "        return self.continuous_features[row_start:row_end, col_idx]\n",
    "\n",
    "    def features_to_tensor(self, features: pd.DataFrame):\n",
    "        result = features.to_numpy()\n",
    "        # TODO: May need a type conversion\n",
    "        # Split vertically into batches, then concat horizontally so \n",
    "        # time is along vertical axis and features are columns\n",
    "        v_split_out = np.vsplit(result, self.batch_size)\n",
    "        result = torch.tensor(np.hstack(v_split_out)).to(device)\n",
    "        return result\n",
    "    \n",
    "    def fit_count_scaler(self, taxi_data: pd.DataFrame):\n",
    "        value_cols = [\"counts\"]\n",
    "        counts = taxi_data.pivot(columns=\"PULocationID\", index=\"pickup_datetime\", values=value_cols)\n",
    "        counts_scaled = self.fit_scaler(self.count_scaler, counts)\n",
    "        return taxi_data.drop(value_cols, axis=1).merge(counts_scaled, on=[\"PULocationID\", \"pickup_datetime\"])\n",
    "\n",
    "    def fit_other_scaler(self, taxi_data: pd.DataFrame):\n",
    "        value_cols = ['tip_amount', 'fare_amount', 'trip_distance', 'trip_duration']\n",
    "        other_vars = taxi_data.pivot(columns=\"PULocationID\", index=\"pickup_datetime\", values=value_cols)\n",
    "        other_vars_scaled = self.fit_scaler(self.other_scaler, other_vars)\n",
    "        return taxi_data.drop(value_cols, axis=1).merge(other_vars_scaled, on=[\"PULocationID\", \"pickup_datetime\"])\n",
    "\n",
    "    def fit_scaler(self, scaler: MinMaxScaler, pivoted_data: pd.DataFrame):\n",
    "        # Time is along vertical axis\n",
    "        # The pivot columns should be in order but not sure what guarantees that\n",
    "        if self.use_alternate_scaler:\n",
    "            mat = scaler.transform(pivoted_data)\n",
    "        else:\n",
    "            mat = scaler.fit_transform(pivoted_data)\n",
    "        scaled = pd.DataFrame(mat)\n",
    "        scaled.columns = pivoted_data.columns\n",
    "        scaled.index = pivoted_data.index\n",
    "        scaled = scaled.stack(future_stack=True).reset_index()\n",
    "        return scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_timestamp = '2023-10-19 14:00:00-0400'\n",
    "def split_taxi_data_on_timestamp(taxi_dataset: pd.DataFrame, split_timestamp: str):\n",
    "    train_set = taxi_dataset[taxi_dataset[\"pickup_datetime\"] < split_timestamp].copy()\n",
    "    validation_set = taxi_dataset[taxi_dataset[\"pickup_datetime\"] >= split_timestamp].copy()\n",
    "    return train_set, validation_set\n",
    "\n",
    "def create_datasets(train_df, validation_df, sequence_length=24, continuous_features=False, time_features=False):\n",
    "    train_set = TaxiDataset(train_df, sequence_length, continuous_features, time_features)\n",
    "    validation_set = TaxiDataset(validation_df, sequence_length, continuous_features, time_features, use_alternate_scaler=True, alternate_scaler_source=train_set)\n",
    "    return train_set, validation_set\n",
    "\n",
    "def create_dataloaders(train_set, validation_set):\n",
    "    train_loader = DataLoader(train_set, batch_size=train_set.batch_size, shuffle=False, drop_last=True)\n",
    "    validation_loader = DataLoader(validation_set, batch_size=train_set.batch_size, shuffle=False, drop_last=True)\n",
    "    return train_loader, validation_loader\n",
    "\n",
    "def create_unbatched_loader(dataset: TaxiDataset):\n",
    "    dataset.set_max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = split_taxi_data_on_timestamp(zero_taxi_df, split_timestamp)\n",
    "train_set, validation_set = create_datasets(train_df, validation_df, sequence_length, continuous_features=True, time_features=True)\n",
    "train_loader, validation_loader = create_dataloaders(train_set, validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelType(Enum):\n",
    "    RNN = 1\n",
    "    LSTM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "class BaselineModel(nn.Module):\n",
    "    savable = False\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.fc = nn.Linear(3, 10)\n",
    "    \n",
    "    def forward(self, _, continuous: torch.tensor):\n",
    "        constant_pred = continuous[:, :, 0].detach().clone()\n",
    "        constant_pred.requires_grad_()\n",
    "        return constant_pred\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSeriesModel(nn.Module):\n",
    "    savable = True\n",
    "    def __init__(\n",
    "            self, \n",
    "            zone_count, \n",
    "            model_type: ModelType, \n",
    "            batch_size: int, \n",
    "            input_size=1, \n",
    "            hidden_size=50, \n",
    "            output_size=1, \n",
    "            num_layers=1\n",
    "        ):\n",
    "        super(MultiSeriesModel, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.embed_size = embed_size(zone_count)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.zone_embed = nn.Embedding(zone_count, self.embed_size)\n",
    "        \n",
    "        if self.model_type == ModelType.RNN:\n",
    "            self.cell = nn.RNN(self.embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.h = torch.zeros(num_layers, batch_size, hidden_size, device=device)\n",
    "        elif self.model_type == ModelType.LSTM:\n",
    "            self.cell = nn.LSTM(self.embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.h = [torch.zeros(num_layers, batch_size, hidden_size, device=device) for _ in range(2)]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model only supports RNN and LSTM\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, zones, continuous):\n",
    "        # Zones is (zone_count, 1), emb (zone_count, embed_size(zone_count))\n",
    "        sequence_length = continuous.shape[-2]\n",
    "        embed_result = self.zone_embed(zones).expand((-1, sequence_length, -1))\n",
    "        x = torch.cat([embed_result, continuous], dim=-1)\n",
    "        if self.model_type == ModelType.RNN:\n",
    "            out, h = self.cell(x, self.h)\n",
    "            self.h = h.detach()\n",
    "        elif self.model_type == ModelType.LSTM:\n",
    "            out, h = self.cell(x, self.h)\n",
    "            self.h = [h_.detach() for h_ in h]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model only supports RNN and LSTM\")\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out.squeeze()\n",
    "\n",
    "    def reset(self):\n",
    "        if type(self.h) is list:\n",
    "            for h in self.h: \n",
    "                h.zero_()\n",
    "        else:\n",
    "            self.h.zero_()\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        if self.input_size > 1:\n",
    "            return f\"{self.model_type.name}_MultiSeriesModel\"\n",
    "        else:\n",
    "            return f\"{self.model_type.name}_SingleSeriesModel\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "class MultiSeriesTimeModel(nn.Module):\n",
    "    savable = True\n",
    "    def __init__(\n",
    "            self, \n",
    "            zone_count,\n",
    "            int_features: Dict[int, int],  # index and number of distinct values\n",
    "            model_type: ModelType, \n",
    "            batch_size: int, \n",
    "            input_size=1, \n",
    "            hidden_size=50, \n",
    "            output_size=1, \n",
    "            num_layers=1\n",
    "        ):\n",
    "        super(MultiSeriesTimeModel, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.embed_size = embed_size(zone_count)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.zone_embed = nn.Embedding(zone_count, self.embed_size)\n",
    "\n",
    "        self.int_embeds = {}\n",
    "\n",
    "        int_embed_size = 0\n",
    "        for idx, num_unique in int_features.items():\n",
    "            self.int_embeds[idx] = nn.Embedding(num_unique, embed_size(num_unique)).to(device)\n",
    "            int_embed_size += embed_size(num_unique)\n",
    "\n",
    "\n",
    "        recurrent_size = self.embed_size + input_size + int_embed_size\n",
    "        if self.model_type == ModelType.RNN:\n",
    "            self.cell = nn.RNN(recurrent_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.h = torch.zeros(num_layers, batch_size, hidden_size, device=device)\n",
    "        elif self.model_type == ModelType.LSTM:\n",
    "            self.cell = nn.LSTM(recurrent_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.h = [torch.zeros(num_layers, batch_size, hidden_size, device=device) for _ in range(2)]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model only supports RNN and LSTM\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, zones, continuous, int_features):\n",
    "        # Zones is (zone_count, 1), emb (zone_count, embed_size(zone_count))\n",
    "        sequence_length = continuous.shape[-2]\n",
    "        embed_result = self.zone_embed(zones).expand((-1, sequence_length, -1))\n",
    "\n",
    "        int_embed_results = {}\n",
    "        for idx, embed in self.int_embeds.items():\n",
    "            int_embed_results[idx] = embed(int_features[:, :, idx])\n",
    "\n",
    "        embed_result_arr = []\n",
    "        for idx in sorted(list(int_embed_results.keys())):\n",
    "            embed_result_arr.append(int_embed_results[idx])\n",
    "\n",
    "        x = torch.cat(embed_result_arr + [embed_result, continuous], dim=-1)\n",
    "        if self.model_type == ModelType.RNN:\n",
    "            out, h = self.cell(x, self.h)\n",
    "            self.h = h.detach()\n",
    "        elif self.model_type == ModelType.LSTM:\n",
    "            out, h = self.cell(x, self.h)\n",
    "            self.h = [h_.detach() for h_ in h]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model only supports RNN and LSTM\")\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out.squeeze()\n",
    "\n",
    "    def reset(self):\n",
    "        if type(self.h) is list:\n",
    "            for h in self.h: \n",
    "                h.zero_()\n",
    "        else:\n",
    "            self.h.zero_()\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        if self.input_size > 1:\n",
    "            return f\"{self.model_type.name}_MultiSeriesTimeModel\"\n",
    "        else:\n",
    "            return f\"{self.model_type.name}_SingleSeriesTimeModel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLoop:\n",
    "    NUM_EPOCHS = 1000\n",
    "    MODEL_FOLDER = \"../models/model_validation/\"\n",
    "    PRINT_EVERY = 10\n",
    "\n",
    "    def __init__(self, model, train_loader, validation_loader):\n",
    "        self.model: nn.Module = model\n",
    "        self.optimizer: optim.Optimizer = optim.Adam(model.parameters(), lr=3e-3)\n",
    "        self.criterion: nn.MSELoss = nn.MSELoss()\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "        self.min_valid_loss = float(\"inf\")\n",
    "        self.best_model_path = \"\"\n",
    "\n",
    "        self.valid_loss_list = []\n",
    "        self.consecutive_loss_increases = 0\n",
    "\n",
    "\n",
    "    def train_epoch(self) -> float:\n",
    "        self.model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for *vars, target in self.train_loader:\n",
    "            preds = self.model(*vars)\n",
    "            train_loss = self.criterion(preds, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_train_loss += train_loss.item()\n",
    "        self.model.reset()\n",
    "\n",
    "        avg_train_loss = epoch_train_loss / len(self.train_loader)\n",
    "        return avg_train_loss\n",
    "\n",
    "    def validate_epoch(self) -> float:\n",
    "        self.model.eval()\n",
    "        epoch_valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for *vars, target in self.validation_loader:\n",
    "                preds = self.model(*vars)            \n",
    "                valid_loss = self.criterion(preds, target)\n",
    "\n",
    "                epoch_valid_loss += valid_loss.item()\n",
    "        self.model.reset()\n",
    "        \n",
    "        avg_valid_loss = epoch_valid_loss / len(self.validation_loader)\n",
    "        return avg_valid_loss\n",
    "    \n",
    "    def print_loss(self, epoch, train_loss, valid_loss):\n",
    "        print(f'Epoch [{epoch + 1:04}/{self.NUM_EPOCHS}], Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "    \n",
    "    def fit_one_epoch(self):\n",
    "        avg_train_loss = self.train_epoch()\n",
    "        avg_valid_loss = self.validate_epoch()\n",
    "        return avg_train_loss, avg_valid_loss\n",
    "    \n",
    "    def save_model(self, epoch) -> str:\n",
    "        if self.model.savable:\n",
    "            model_name = f\"{self.model.get_model_name()}_{self.timestamp}_{epoch}\"\n",
    "            model_path = self.MODEL_FOLDER + model_name\n",
    "            torch.save(self.model.state_dict(), model_path)\n",
    "            return model_path\n",
    "\n",
    "    def handle_early_stopping(self, epoch) -> bool:\n",
    "        stop = False\n",
    "        if self.valid_loss_list[-1] < self.min_valid_loss:\n",
    "            # Save model\n",
    "            self.min_valid_loss = self.valid_loss_list[-1]\n",
    "            self.best_model_path = self.save_model(epoch)\n",
    "            # Reset count\n",
    "            self.consecutive_loss_increases = 0\n",
    "        else:\n",
    "            self.consecutive_loss_increases += 1\n",
    "            if self.consecutive_loss_increases >= 20:\n",
    "                stop = True\n",
    "\n",
    "        return stop\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.NUM_EPOCHS):\n",
    "            avg_train_loss, avg_valid_loss = self.fit_one_epoch()\n",
    "\n",
    "            if (epoch + 1) % self.PRINT_EVERY == 0:\n",
    "                self.print_loss(epoch, avg_train_loss, avg_valid_loss)\n",
    "\n",
    "            ## Early Stopping: check if the error went up or down\n",
    "            self.valid_loss_list.append(avg_valid_loss)\n",
    "            stop = self.handle_early_stopping(epoch)\n",
    "            if stop or not self.model.savable:\n",
    "                break\n",
    "        if self.model.savable:\n",
    "            self.model.load_state_dict(torch.load(self.best_model_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better than baseline !!!\n",
    "# 0.08\n",
    "# np.mean(np.power(np.array(train_set.continuous_features.cpu()[:-1, :] - train_set.continuous_features.cpu()[1:, :]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            train_set: TaxiDataset,\n",
    "            validation_set: TaxiDataset\n",
    "        ) -> None:\n",
    "        validation_set.set_max_sequence_length()\n",
    "        self.train_set = train_set\n",
    "        self.validation_set = validation_set\n",
    "        self.validation_loader = DataLoader(validation_set, validation_set.batch_size, shuffle=False)\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "        # TODO: Validation set needs to be scaled by train set scalers\n",
    "        \n",
    "    def get_preds(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for *vars, target in self.validation_loader:\n",
    "                preds = self.model(*vars)\n",
    "        self.model.reset()\n",
    "        # Transpose so that time is along vertical\n",
    "        preds = preds.cpu().detach().numpy().T\n",
    "        target = target.cpu().detach().numpy().T\n",
    "        \n",
    "        return preds, target\n",
    "    \n",
    "    def unscale_preds_and_target(self, scaled_preds, scaled_target):\n",
    "        preds = self.train_set.count_scaler.inverse_transform(scaled_preds)\n",
    "        target = self.validation_set.count_scaler.inverse_transform(scaled_target)\n",
    "        return preds, target\n",
    "\n",
    "    def mse_loss(self, preds, target):\n",
    "        # mse_loss = nn.MSELoss()\n",
    "        # scaled_loss = mse_loss(preds, target)\n",
    "        # print(type(scaled_loss))\n",
    "        # return scaled_loss.item()\n",
    "        return ((preds - target) ** 2).mean()\n",
    "\n",
    "    def smape_loss(self, preds, target):\n",
    "        # TODO: Eliminiate runtime warning when denominator == 0\n",
    "        smape_mat = (2 * np.abs(preds - target)) / (np.abs(preds) + np.abs(target))\n",
    "        smape_mat = np.mean(np.nan_to_num(smape_mat))\n",
    "        return smape_mat\n",
    "\n",
    "    def validate(self):\n",
    "        scaled_preds, scaled_target = self.get_preds()\n",
    "        preds, target = self.unscale_preds_and_target(scaled_preds, scaled_target)\n",
    "        \n",
    "        self.preds = preds\n",
    "        self.target = target\n",
    "        self.scaled_mse_loss = self.mse_loss(scaled_preds, scaled_target)\n",
    "        self.unscaled_mse_loss = self.mse_loss(preds, target)\n",
    "        self.smape_loss = self.smape_loss(preds, target)\n",
    "\n",
    "    def print_results(self):\n",
    "        print(f\"Scaled MSE: {self.scaled_mse_loss:.4f}, Unscaled MSE: {self.unscaled_mse_loss}, sMAPE: {self.smape_loss}\")\n",
    "    \n",
    "    # TODO: Some kind of plot support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelContainer:\n",
    "    # Act as a container for all of the other models\n",
    "    def __init__(self, taxi_df, model_type, sequence_length, continuous_features, time_features) -> None:\n",
    "        # Configuration options\n",
    "        self.continuous_features = continuous_features\n",
    "        self.time_features = time_features\n",
    "\n",
    "        self.data = taxi_df\n",
    "        self.model_type = model_type\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Setup\n",
    "        self.create_datasets()\n",
    "        self.setup_model()\n",
    "\n",
    "    def run(self):\n",
    "        self.train_model()\n",
    "        self.validate_model()\n",
    "\n",
    "    def create_datasets(self):\n",
    "        train_df, validation_df = split_taxi_data_on_timestamp(self.data, split_timestamp)\n",
    "        train_set, validation_set = create_datasets(\n",
    "            train_df, validation_df, self.sequence_length, \n",
    "            self.continuous_features, self.time_features)\n",
    "\n",
    "        self.train_set = train_set\n",
    "\n",
    "        train_loader, validation_loader = create_dataloaders(train_set, validation_set)\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "\n",
    "        self.validation_set_for_validation = TaxiDataset(validation_df, self.sequence_length, self.continuous_features, self.time_features)\n",
    "\n",
    "    def setup_model(self):\n",
    "        if self.model_type == 'rnn':\n",
    "            model_type = ModelType.RNN\n",
    "        elif self.model_type == 'lstm':\n",
    "            model_type = ModelType.LSTM\n",
    "        else:\n",
    "            raise ValueError(f\"mode_type {self.model_type} not supported\")\n",
    "        \n",
    "\n",
    "        if self.model_type == 'baseline':\n",
    "            self.model = BaselineModel()\n",
    "        elif self.time_features:\n",
    "            self.model = MultiSeriesTimeModel(train_set.num_zones, train_set.int_features_unique,  model_type, train_set.batch_size, self.train_set.num_continuous_features).to(device)\n",
    "        else:\n",
    "            self.model = MultiSeriesModel(train_set.num_zones, model_type, train_set.batch_size, self.train_set.num_continuous_features).to(device)\n",
    "\n",
    "    def train_model(self):\n",
    "        loop = TrainingLoop(self.model, self.train_loader, self.validation_loader)\n",
    "        loop.train()\n",
    "        \n",
    "    def validate_model(self):\n",
    "        valid = Validator(self.model, self.train_set, self.validation_set_for_validation)\n",
    "        valid.validate()\n",
    "        valid.print_results()\n",
    "    \n",
    "    # Dataset generation\n",
    "    # Create the Model\n",
    "    # Fit the model\n",
    "    # Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0057, Validation Loss: 0.0103\n",
      "Epoch [0020/1000], Train Loss: 0.0051, Validation Loss: 0.0057\n",
      "Epoch [0030/1000], Train Loss: 0.0042, Validation Loss: 0.0056\n",
      "Epoch [0040/1000], Train Loss: 0.0042, Validation Loss: 0.0059\n",
      "Epoch [0050/1000], Train Loss: 0.0038, Validation Loss: 0.0042\n",
      "Epoch [0060/1000], Train Loss: 0.0038, Validation Loss: 0.0044\n",
      "Epoch [0070/1000], Train Loss: 0.0037, Validation Loss: 0.0042\n",
      "Epoch [0080/1000], Train Loss: 0.0036, Validation Loss: 0.0041\n",
      "Epoch [0090/1000], Train Loss: 0.0036, Validation Loss: 0.0040\n",
      "Epoch [0100/1000], Train Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch [0110/1000], Train Loss: 0.0035, Validation Loss: 0.0040\n",
      "Epoch [0120/1000], Train Loss: 0.0035, Validation Loss: 0.0039\n",
      "Epoch [0130/1000], Train Loss: 0.0035, Validation Loss: 0.0039\n",
      "Epoch [0140/1000], Train Loss: 0.0034, Validation Loss: 0.0039\n",
      "Epoch [0150/1000], Train Loss: 0.0034, Validation Loss: 0.0040\n",
      "Scaled MSE: 0.0051, Unscaled MSE: 271.08770751953125, sMAPE: 0.5536814332008362\n"
     ]
    }
   ],
   "source": [
    "# Varies by 10\n",
    "test = ModelContainer(mean_taxi_df, 'lstm', 96, True, True)\n",
    "test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0050, Validation Loss: 0.0050\n",
      "Epoch [0020/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0030/1000], Train Loss: 0.0045, Validation Loss: 0.0045\n",
      "Epoch [0040/1000], Train Loss: 0.0044, Validation Loss: 0.0044\n",
      "Epoch [0050/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0060/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0070/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0080/1000], Train Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch [0090/1000], Train Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch [0100/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0110/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0120/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0130/1000], Train Loss: 0.0039, Validation Loss: 0.0041\n",
      "Epoch [0140/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch [0150/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch [0160/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch [0170/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Epoch [0180/1000], Train Loss: 0.0038, Validation Loss: 0.0040\n",
      "Scaled MSE: 0.0059, Unscaled MSE: 374.28741455078125, sMAPE: 0.5773265361785889\n"
     ]
    }
   ],
   "source": [
    "test = ModelContainer(mean_taxi_df, 'lstm', 96, True, False)\n",
    "test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0053, Validation Loss: 0.0075\n",
      "Epoch [0020/1000], Train Loss: 0.0044, Validation Loss: 0.0051\n",
      "Epoch [0030/1000], Train Loss: 0.0040, Validation Loss: 0.0046\n",
      "Epoch [0040/1000], Train Loss: 0.0039, Validation Loss: 0.0046\n",
      "Epoch [0050/1000], Train Loss: 0.0038, Validation Loss: 0.0044\n",
      "Epoch [0060/1000], Train Loss: 0.0037, Validation Loss: 0.0043\n",
      "Epoch [0070/1000], Train Loss: 0.0036, Validation Loss: 0.0043\n",
      "Epoch [0080/1000], Train Loss: 0.0036, Validation Loss: 0.0042\n",
      "Epoch [0090/1000], Train Loss: 0.0036, Validation Loss: 0.0042\n",
      "Epoch [0100/1000], Train Loss: 0.0035, Validation Loss: 0.0041\n",
      "Epoch [0110/1000], Train Loss: 0.0035, Validation Loss: 0.0041\n",
      "Epoch [0120/1000], Train Loss: 0.0034, Validation Loss: 0.0041\n",
      "Epoch [0130/1000], Train Loss: 0.0034, Validation Loss: 0.0041\n",
      "Epoch [0140/1000], Train Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch [0150/1000], Train Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch [0160/1000], Train Loss: 0.0034, Validation Loss: 0.0040\n",
      "Epoch [0170/1000], Train Loss: 0.0033, Validation Loss: 0.0040\n",
      "Epoch [0180/1000], Train Loss: 0.0033, Validation Loss: 0.0040\n",
      "Epoch [0190/1000], Train Loss: 0.0033, Validation Loss: 0.0040\n",
      "Scaled MSE: 0.0049, Unscaled MSE: 251.4924774169922, sMAPE: 0.5780544281005859\n"
     ]
    }
   ],
   "source": [
    "test = ModelContainer(mean_taxi_df, 'lstm', 96, False, True)\n",
    "test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0010/1000], Train Loss: 0.0050, Validation Loss: 0.0049\n",
      "Epoch [0020/1000], Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Epoch [0030/1000], Train Loss: 0.0044, Validation Loss: 0.0045\n",
      "Epoch [0040/1000], Train Loss: 0.0043, Validation Loss: 0.0044\n",
      "Epoch [0050/1000], Train Loss: 0.0042, Validation Loss: 0.0043\n",
      "Epoch [0060/1000], Train Loss: 0.0041, Validation Loss: 0.0042\n",
      "Epoch [0070/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0080/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0090/1000], Train Loss: 0.0040, Validation Loss: 0.0041\n",
      "Epoch [0100/1000], Train Loss: 0.0040, Validation Loss: 0.0042\n",
      "Epoch [0110/1000], Train Loss: 0.0039, Validation Loss: 0.0042\n",
      "Scaled MSE: 0.0052, Unscaled MSE: 274.9544982910156, sMAPE: 0.5505231618881226\n"
     ]
    }
   ],
   "source": [
    "test = ModelContainer(mean_taxi_df, 'lstm', 96, False, False)\n",
    "test.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
